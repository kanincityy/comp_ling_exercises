{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"a2JRTgqKRqqL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737622095651,"user_tz":0,"elapsed":28194,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"86cfa832-dedd-4460-e15e-6c4905a9b25f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting otter-grader\n","  Downloading otter_grader-6.0.4-py3-none-any.whl.metadata (3.9 kB)\n","Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (8.1.8)\n","Collecting dill>=0.3.0 (from otter-grader)\n","  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n","Collecting fica>=0.4.1 (from otter-grader)\n","  Downloading fica-0.4.1-py3-none-any.whl.metadata (2.0 kB)\n","Collecting ipylab<2.0.0,>=1.0.0 (from otter-grader)\n","  Downloading ipylab-1.0.0-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from otter-grader) (7.34.0)\n","Collecting ipywidgets<9.0.0,>=8.1.5 (from otter-grader)\n","  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: jinja2<4.0,>=3.1 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (3.1.5)\n","Collecting jupytext<2.0.0,>=1.16.4 (from otter-grader)\n","  Downloading jupytext-1.16.6-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: nbconvert>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (7.16.5)\n","Requirement already satisfied: nbformat>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (5.10.4)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (2.2.2)\n","Collecting python-on-whales<1.0.0,>=0.72.0 (from otter-grader)\n","  Downloading python_on_whales-0.75.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: pyyaml<7,>=6 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (6.0.2)\n","Requirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (2.32.3)\n","Requirement already satisfied: wrapt<2.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (1.17.0)\n","Requirement already satisfied: docutils in /usr/local/lib/python3.11/dist-packages (from fica>=0.4.1->otter-grader) (0.21.2)\n","Requirement already satisfied: sphinx in /usr/local/lib/python3.11/dist-packages (from fica>=0.4.1->otter-grader) (8.1.3)\n","Collecting comm>=0.1.3 (from ipywidgets<9.0.0,>=8.1.5->otter-grader)\n","  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9.0.0,>=8.1.5->otter-grader) (5.7.1)\n","Collecting widgetsnbextension~=4.0.12 (from ipywidgets<9.0.0,>=8.1.5->otter-grader)\n","  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n","Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9.0.0,>=8.1.5->otter-grader) (3.0.13)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (75.1.0)\n","Collecting jedi>=0.16 (from ipython->otter-grader)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (3.0.48)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (4.9.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0,>=3.1->otter-grader) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=1.0 in /usr/local/lib/python3.11/dist-packages (from jupytext<2.0.0,>=1.16.4->otter-grader) (3.0.0)\n","Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from jupytext<2.0.0,>=1.16.4->otter-grader) (0.4.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from jupytext<2.0.0,>=1.16.4->otter-grader) (24.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (4.12.3)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (6.2.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.7.1)\n","Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (5.7.2)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.3.0)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (3.1.0)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.10.2)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (1.5.1)\n","Collecting playwright (from nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader)\n","  Downloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.0.0->otter-grader) (2.21.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.0.0->otter-grader) (4.23.0)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->otter-grader) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->otter-grader) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->otter-grader) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->otter-grader) (2024.2)\n","Requirement already satisfied: pydantic!=2.0.*,<3,>=2 in /usr/local/lib/python3.11/dist-packages (from python-on-whales<1.0.0,>=0.72.0->otter-grader) (2.10.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from python-on-whales<1.0.0,>=0.72.0->otter-grader) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->otter-grader) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->otter-grader) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->otter-grader) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->otter-grader) (2024.12.14)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.5.1)\n","Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (1.4.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->otter-grader) (0.8.4)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (24.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (0.22.3)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (4.3.6)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=1.0->jupytext<2.0.0,>=1.16.4->otter-grader) (0.1.2)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from nbclient>=0.5.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (6.1.12)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->otter-grader) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader) (0.2.13)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,<3,>=2->python-on-whales<1.0.0,>=0.72.0->otter-grader) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,<3,>=2->python-on-whales<1.0.0,>=0.72.0->otter-grader) (2.27.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->otter-grader) (1.17.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (2.6)\n","Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (3.1.1)\n","Collecting pyee==12.0.0 (from playwright->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader)\n","  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.0.0)\n","Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.0.0)\n","Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.1.0)\n","Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (1.0.1)\n","Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.0.0)\n","Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.0.0)\n","Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.2.0)\n","Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.16.0)\n","Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (1.0.0)\n","Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (1.4.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (24.0.1)\n","Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (6.3.3)\n","Downloading otter_grader-6.0.4-py3-none-any.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.7/141.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fica-0.4.1-py3-none-any.whl (13 kB)\n","Downloading ipylab-1.0.0-py3-none-any.whl (100 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.2/100.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jupytext-1.16.6-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.2/154.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_on_whales-0.75.1-py3-none-any.whl (114 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl (44.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyee-12.0.0-py3-none-any.whl (14 kB)\n","Installing collected packages: widgetsnbextension, pyee, jedi, dill, comm, playwright, python-on-whales, ipywidgets, fica, ipylab, jupytext, otter-grader\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.10\n","    Uninstalling widgetsnbextension-3.6.10:\n","      Successfully uninstalled widgetsnbextension-3.6.10\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","Successfully installed comm-0.2.2 dill-0.3.9 fica-0.4.1 ipylab-1.0.0 ipywidgets-8.1.5 jedi-0.19.2 jupytext-1.16.6 otter-grader-6.0.4 playwright-1.49.1 pyee-12.0.0 python-on-whales-0.75.1 widgetsnbextension-4.0.13\n","--2025-01-23 08:48:15--  https://sebschu.com/files/plin0072/tutorials/tutorial2/tests.zip\n","Resolving sebschu.com (sebschu.com)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...\n","Connecting to sebschu.com (sebschu.com)|185.199.110.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16791 (16K) [application/zip]\n","Saving to: ‘tests.zip’\n","\n","tests.zip           100%[===================>]  16.40K  --.-KB/s    in 0s      \n","\n","2025-01-23 08:48:15 (105 MB/s) - ‘tests.zip’ saved [16791/16791]\n","\n","Archive:  tests.zip\n","   creating: tests/\n","  inflating: tests/q5.py             \n","  inflating: tests/q4.py             \n","  inflating: tests/q1b.py            \n","  inflating: tests/q1a.py            \n","  inflating: tests/q3.py             \n","  inflating: tests/q7.py             \n","  inflating: tests/q6.py             \n","  inflating: tests/q2.py             \n"]}],"source":["!pip install otter-grader\n","!wget \"https://sebschu.com/files/plin0072/tutorials/tutorial2/tests.zip\" && unzip -o tests.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"0a6M3aGvRqqN"},"outputs":[],"source":["# Initialize Otter\n","import otter\n","grader = otter.Notebook()"]},{"cell_type":"markdown","metadata":{"id":"TMJK-YJ0RGvK"},"source":["\n","# Tutorial 2: Word vectors\n","\n","In this tutorial, you will practice working with word vectors loaded from a word embedding file. You will build a part-of-speech tagger that uses word vectors as features.\n","\n","### Setup\n","\n","Please first make a copy of this notebook into your personal Google Drive. You can do so by clicking on _File_ (top left corner) and choosing \"Save a copy in Drive\". Then, make sure that you run the two cells on top of the notebook before running any of the other cells. Otherwise you will not be able to run the automatic tests.\n"]},{"cell_type":"markdown","metadata":{"id":"FzElLRGhLOUT"},"source":["## A part-of-speech tagger with word vectors\n","\n","In [PLIN0034 Lecture 7](https://colab.research.google.com/drive/1u0j5OFJ-M1qTQ-JxHaw-X8WFJqgWZNKq?usp=sharing#scrollTo=RmygCNOw7pJ6), we built a part-of-speech tagger using a multinomial logistic regression classifier. This part-of-speech tagger was trained to assign a part-of-speech tag (e.g., `NOUN` or `VERB` to each word in the sentence).\n","\n","In the original version of the tagger, we used several indicator features, namely:  \n","\n","1. Identity of the previous word. If the word to be tagged is the first word in the sentence, the feature will be the special \"FIRST_WORD\" token.\n","2. Identity of the word to be tagged.\n","3. Identity of the next word. If the word to be tagged is the last word in the sentence, the feature will be the special \"LAST_WORD\" token.\n","4. The last three characters of the word. The idea behind this feature is that the suffix of a word in English often indicates what part of speech the word should have. For example, \"-ion\" often indicates a noun, \"-ly\" adverbs, \"-ise\" verbs, etc. With this feature the classifier will be able to make a more educated guess even if it hasn't seen the specific word in the training data.\n","\n","One potential issue with this tagger is that if there are words that it hasn't seen in the training data, it will likely make mistakes in their classification.\n","\n","In this exercise, you will therefore build a new classifier that uses word vectors along with the identity of words as features. This will ideally generalise better: Even if we haven't seen a word in the training data, the word vector of the new word will be very similar to words we have seen in the training data and the classifier will have more information to work of."]},{"cell_type":"markdown","metadata":{"id":"LmAnJKAtJpBW"},"source":["### Original classifier\n","\n","We'll first load the data and build the original classifier. This is already implemented below. You will then implement the classifier that uses word vectors and compare its performance to the original classifier. If you would like to read more detailed explanations for each step, please refer to the [PLIN0034 Lecture 7](https://colab.research.google.com/drive/1u0j5OFJ-M1qTQ-JxHaw-X8WFJqgWZNKq?usp=sharing#scrollTo=RmygCNOw7pJ6) Colab notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"toY7mF0rJ6O3"},"outputs":[],"source":["# A function to read in CoNLL-U files, which contain sentences annotated with\n","# their part-of-speech tags.\n","# See https://universaldependencies.org/format.html for more info on this file\n","# format.\n","\n","def read_conllu_file(file_path):\n","  sentences = []\n","\n","  with open(file_path, \"r\", encoding=\"UTF-8\") as in_f:\n","    current_sentence = []\n","    for line in in_f:\n","      line = line.strip()\n","      # ignore lines starting with # (comments)\n","      if line.startswith(\"#\"):\n","        continue\n","\n","      # an empty line indicates the end of the sentence\n","      if line == \"\":\n","        sentences.append(current_sentence)\n","        current_sentence = []\n","        continue\n","\n","      # split the line into its parts\n","      parts = line.split(\"\\t\")\n","\n","      # extract the index (the first column)\n","      idx = parts[0]\n","\n","      # check if this is a multi-word token or an empty node\n","      if \".\" in idx or \"-\" in idx:\n","        continue\n","\n","      if len(parts) < 4:\n","        print(parts)\n","      # extract the word and the tag, i.e., the second and fourth column\n","      word, tag = parts[1], parts[3]\n","\n","      # append the word, tag pair to the current sentence\n","      current_sentence.append((word, tag))\n","\n","  return sentences\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zvCEIBA6KSDS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737622179880,"user_tz":0,"elapsed":991,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"72efe0d8-c02b-4b1d-e1c2-f6b3c4fce9de"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-01-23 08:49:38--  https://github.com/UniversalDependencies/UD_English-EWT/raw/refs/heads/master/en_ewt-ud-train.conllu\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/refs/heads/master/en_ewt-ud-train.conllu [following]\n","--2025-01-23 08:49:39--  https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/refs/heads/master/en_ewt-ud-train.conllu\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14190227 (14M) [text/plain]\n","Saving to: ‘en_ewt-ud-train.conllu’\n","\n","en_ewt-ud-train.con 100%[===================>]  13.53M  --.-KB/s    in 0.07s   \n","\n","2025-01-23 08:49:39 (185 MB/s) - ‘en_ewt-ud-train.conllu’ saved [14190227/14190227]\n","\n","--2025-01-23 08:49:39--  https://github.com/UniversalDependencies/UD_English-EWT/raw/refs/heads/master/en_ewt-ud-dev.conllu\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/refs/heads/master/en_ewt-ud-dev.conllu [following]\n","--2025-01-23 08:49:39--  https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/refs/heads/master/en_ewt-ud-dev.conllu\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1804172 (1.7M) [text/plain]\n","Saving to: ‘en_ewt-ud-dev.conllu’\n","\n","en_ewt-ud-dev.conll 100%[===================>]   1.72M  --.-KB/s    in 0.01s   \n","\n","2025-01-23 08:49:39 (159 MB/s) - ‘en_ewt-ud-dev.conllu’ saved [1804172/1804172]\n","\n"]}],"source":["# Download annotated files. This downloads files for English but you can also change the paths to download files for other languages.\n","# See the repositories in https://github.com/UniversalDependencies for other languages.\n","! mkdir -p data && cd data && wget https://github.com/UniversalDependencies/UD_English-EWT/raw/refs/heads/master/en_ewt-ud-train.conllu\n","! cd data && wget https://github.com/UniversalDependencies/UD_English-EWT/raw/refs/heads/master/en_ewt-ud-dev.conllu\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ID_6Vp4pKpXF"},"outputs":[],"source":["# a function for extracting the identity features and labels for a sentence\n","def extract_features(sentences):\n","\n","  labels = []\n","  extracted_features = []\n","\n","  for sentence in sentences:\n","    n_words = len(sentence)\n","    for i in range(n_words):\n","      word, tag = sentence[i]\n","      word = word.lower()\n","      labels.append(tag)\n","\n","      # extract the previous word (if it exists)\n","      if i > 0:\n","        prev_word, _ = sentence[i-1]\n","        prev_word = prev_word.lower()\n","      else:\n","        prev_word = \"FIRST_WORD\"\n","\n","      # extract the next word (if it exists)\n","      if i < n_words - 1:\n","        next_word, _ = sentence[i+1]\n","        next_word = next_word.lower()\n","      else:\n","        next_word = \"LAST_WORD\"\n","\n","      # extract the suffix (last 3 characters)\n","      suffix = word[-3:]\n","\n","      # turn the features into the following dictionary\n","      # {\n","      #   prev: previous_word,\n","      #   next: next_word,\n","      #   word: word,\n","      #   suffix: suffix\n","      # }\n","      ex_features = {\"prev\": prev_word,\n","                     \"next\":next_word,\n","                     \"word\":word,\n","                     \"suffix\": suffix\n","                     }\n","\n","\n","      extracted_features.append(ex_features)\n","\n","  return extracted_features, labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ELkcsUTK0lQ"},"outputs":[],"source":["# extract features from the training data and create a sparse feature matrix\n","train_data_sentences = read_conllu_file(\"data/en_ewt-ud-train.conllu\")\n","\n","# extract the features\n","# train_raw_features is a list of dictionaries with the features for each example\n","# train_labels is a list of labels with one entry per example\n","train_raw_features, train_labels = extract_features(train_data_sentences)\n","\n","# turn the features into a sparse feature matrix\n","from sklearn.feature_extraction import DictVectorizer\n","vectoriser = DictVectorizer()\n","# Extract the mapping from feature names to feature indices\n","# and transform into a sparse feature matrix.\n","train_features = vectoriser.fit_transform(train_raw_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1wFwhDwLHnM","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1737622263538,"user_tz":0,"elapsed":52913,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"8d728e85-eaa9-4246-fa01-f7e410c17506"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(max_iter=4000)"],"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: #000;\n","  --sklearn-color-text-muted: #666;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: flex;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","  align-items: start;\n","  justify-content: space-between;\n","  gap: 0.5em;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label .caption {\n","  font-size: 0.6rem;\n","  font-weight: lighter;\n","  color: var(--sklearn-color-text-muted);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 0.5em;\n","  text-align: center;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=4000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=4000)</pre></div> </div></div></div></div>"]},"metadata":{},"execution_count":7}],"source":["# train the classifier\n","# (this takes about 2 minutes to run)\n","\n","from sklearn.linear_model import LogisticRegression\n","\n","# Intialise a new (multinomial) logistic classifier.\n","original_classifier = LogisticRegression(max_iter=4000)\n","# Train the classifer\n","original_classifier.fit(train_features, train_labels)"]},{"cell_type":"markdown","metadata":{"id":"8pl9tU9zL7ig"},"source":["Having trained this classifier, we can now evaluate how well it is able to predict part-of-speech tags on the test data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atAGbQPsMFFD"},"outputs":[],"source":["# read the data from the CoNLL-U files\n","eval_data_sentences = read_conllu_file(\"data/en_ewt-ud-dev.conllu\")\n","# extract the features and labels\n","eval_raw_features, eval_labels = extract_features(eval_data_sentences)\n","# transform the features so that we can use them as input to the classifier\n","eval_features = vectoriser.transform(eval_raw_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8HOuEx4PZLnz"},"outputs":[],"source":["# make predictions on the evaluation data\n","eval_preds = original_classifier.predict(eval_features)"]},{"cell_type":"markdown","metadata":{"id":"zOBvhgMFrc9p"},"source":["Finally, we can compute the accuracy of the original classifier. This should be 92.15%."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"giT7m2zrZN5H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737622290209,"user_tz":0,"elapsed":205,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"748eb9fb-96c7-4279-bd7f-5f015e75b44b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of original classifier on test data: 92.15%\n"]}],"source":["correct = 0\n","n_examples = len(eval_preds)\n","for i in range(n_examples):\n","  if eval_labels[i] == eval_preds[i]:\n","    correct += 1\n","\n","print(f\"Accuracy of original classifier on test data: {correct/n_examples * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"uyROGAeTroFy"},"source":["## Using word vectors as features\n","\n","Your task now is to extend the classifier from above so that it also uses word vectors as features. We will input the word vectors by concatenating the word vectors for the previous word, the word we want to tag, and the next word and adding this vector to the existing feature vector of each example.\n","\n","Specifically, you'll have to do the following steps:\n","\n","1. **Load the word vectors** into a numpy matrix and create a dictionary that maps words to the correct row in the matrix.\n","2. **Construct a word vector for unknown words**: Some words may not be in the training data of the word vectors and therefore we need to compute a special *UNK* word vector, which we'll use for unknown words.\n","3. A **feature extractor** that both builds the concatenated vector for each example and builds a dictionary with the indicator features, as above.\n","4. **Process the training data** to obtain the feature matrix for training the classifier.\n","5. **Train the classifier**.\n","6. **Process the evaluation data** to obtin the feature matrix for evaluating the classifier.\n","7. **Compute the accuracy** of the new classifier on the evaluation data."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"AmP-xqaov--m"},"source":["### Step 1: Loading word vectors\n","\n","Run the following cell to download the GloVe embeddings to the folder `glove`."]},{"cell_type":"code","execution_count":4,"metadata":{"deletable":false,"editable":false,"id":"wx4HHt5SaqEU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737719093062,"user_tz":0,"elapsed":243,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"6d4f9495-d1d5-431c-b08e-3d7a335e2724"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘glove’: File exists\n"]}],"source":["# download the embeddings trained on 6B tokens from Wikipedia and the Gigaword news corpus\n","! mkdir glove && cd glove && wget https://nlp.stanford.edu/data/glove.6B.zip && unzip glove.6B.zip"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"T0FGg_cYwUm5"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","In the following cell, load the embeddings using the [`np.loadtxt`](https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html) method. We'll use the 50-dimensional GloVe embeddings stored in `glove/glove.6B.50d.txt`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fJv7qU8a7PX","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737627880420,"user_tz":0,"elapsed":10012,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"50b7046e-afb8-479b-a361-5912b1f148fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded embedding matrix of shape (400000, 100).\n"]}],"source":["import numpy as np\n","\n","NO_OF_DIMENSIONS = 100\n","# TODO load the embeddings\n","# we want to use the second column to the 51st column, which is 1 to 50 given the 0-indexing\n","cols_to_use = list(range(1, NO_OF_DIMENSIONS + 1)) # this returns a list with integers from 1 to 50\n","embeddings = np.loadtxt(\"glove/glove.6B.100d.txt\", encoding=\"UTF-8\", usecols=cols_to_use, comments=None)\n","\n","print(f\"Loaded embedding matrix of shape {embeddings.shape}.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"0PCFOSMDRqqU","colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"status":"ok","timestamp":1737622529472,"user_tz":0,"elapsed":197,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"b0bd59a7-e4ae-477b-b22f-fa3b1216ceab"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["q1a results: All test cases passed!"],"text/html":["<p><strong><pre style='display: inline;'>q1a</pre></strong> passed! 🌟</p>"]},"metadata":{},"execution_count":13}],"source":["grader.check(\"q1a\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"G7wGUxqZxglr"},"source":["Next, build a dictionary `word2index` that maps a word to the row in the embedding matrix. You can do this by iterating through the embedding file and using the first column in the file as the keys to the dictionary and the line number as the value for the dictionary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6sjAqcIa_aK","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737627889059,"user_tz":0,"elapsed":3491,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"6f5b8122-314c-497f-cd4b-939591cb5dce"},"outputs":[{"output_type":"stream","name":"stdout","text":["word2index has 400000 entries.\n"]}],"source":["word2index = {}\n","with open(\"glove/glove.6B.100d.txt\", \"r\", encoding=\"UTF-8\") as embedding_f:\n","  # this loops through the file and returns the line number in the file and the\n","  # contents of the line on each iteration\n","  for i, line in enumerate(embedding_f):\n","    cols = line.split(\" \") # let's split the line into columns\n","    word = cols[0] # the first column contains the word\n","    word2index[word] = i # add the word to the mapping\n","\n","print(f\"word2index has {len(word2index)} entries.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"alkqJUaiRqqU","colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"status":"ok","timestamp":1737622551380,"user_tz":0,"elapsed":193,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"95260ee4-19c1-43d2-828d-0dc6444844e2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["q1b results: All test cases passed!"],"text/html":["<p><strong><pre style='display: inline;'>q1b</pre></strong> passed! ✨</p>"]},"metadata":{},"execution_count":15}],"source":["grader.check(\"q1b\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"zj4SxVtW1cQf"},"source":["### 2. A vector for unknown words\n","\n","Next, we'll compute a vector that we'll use for words that are not in the `word2index` dictionary and hence there is also no vector representation of them.\n","\n","To do this, we'll compute the average across all word vectors in `embeddings`. You can do this using the [`np.mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) method. This method takes a numpy array and a named argument `axis`, which defines along with axis we compute the mean. If you set `axis` to 0, then the function computes a mean across all rows; if you set `axis` to 1, then the function computes a mean across all columns.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4zNcYr8bdbE","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737627892395,"user_tz":0,"elapsed":204,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"fd03378a-3e18-403e-d219-ba5fa8f44751"},"outputs":[{"output_type":"stream","name":"stdout","text":["Computed word vector of shape (100,)\n"]}],"source":["UNK_VECTOR = np.mean(embeddings, axis=0)\n","print(f\"Computed word vector of shape {UNK_VECTOR.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"M2CPjKWsRqqU","colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"status":"ok","timestamp":1737623135836,"user_tz":0,"elapsed":252,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"9286988e-7d12-4d78-971c-c2952272383d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["q2 results: All test cases passed!"],"text/html":["<p><strong><pre style='display: inline;'>q2</pre></strong> passed! ✨</p>"]},"metadata":{},"execution_count":31}],"source":["grader.check(\"q2\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"yk5Hjoo16x21"},"source":["### 3. Feature extraction\n","\n","Next, you'll write a function that does three things:\n","\n","1. store the labels (i.e., the part of speech tag for each word)\n","2. extract the indicator features\n","3. build the vector for each example\n","\n","\n","For step 3, you can use the function `get_vector` to get a vector $v_{word}$ of dimension 50. For each example, construct a vector of dimension 150 $\\left[ v_{prev}\\ v_{current}\\ v_{next} \\right]$ by using `np.hstack`.\n","\n","When there is no previous word or there is no next word, use a 50-dimensional vector with all 0s as the vector for the missing word. You can use [`np.zeros`](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html) to construct such a vector.\n","\n","Once you've constructed the word vector for an example, add it to the list `feature_vectors`.\n","\n","Finally, once we've looped through all examples, we construct the matrix `feature_matrix` by vertically stacking all feature vectors in `feature_vectors`. This will result in a matrix of dimension NUMBER_OF_EXAMPLES x 150."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWDwu9_bajuv","tags":[],"executionInfo":{"status":"error","timestamp":1737642187774,"user_tz":0,"elapsed":244,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"colab":{"base_uri":"https://localhost:8080/","height":193},"outputId":"72305921-72e5-4e54-8ff5-cfa51878fc18"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'parser_config' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-191c80772bbf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Example of using the extract_vector_features_for_parser function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdense_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_vector_features_for_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'parser_config' is not defined"]}],"source":["# This function returns a word vector by looking up\n","# word in the embedding matrix.\n","# if word is not represented in the embedding matrix,\n","# it returns the special UNK_VECTOR instead.\n","def get_vector(word):\n","  word = word.lower()\n","  if word not in word2index:\n","    return UNK_VECTOR\n","  else:\n","    # look up the correct row\n","    idx = word2index[word]\n","    # return the embeddings at row idx\n","    return embeddings[idx, :]\n","\n","\n","def extract_vector_features(sentences):\n","  labels = []\n","  feature_vectors = []\n","  discrete_features = []\n","\n","  # loop through every sentence\n","  for sentence in sentences:\n","    n_words = len(sentence)\n","    # loop through every word\n","    for i in range(n_words):\n","      # get the current word and tag\n","      word, tag = sentence[i]\n","      # append the current word and tag\n","      labels.append(tag)\n","\n","      # TODO: assign the word vector for the current word\n","      word_vector = get_vector(word)\n","\n","      # extract the previous word (if it exists)\n","      if i > 0:\n","        prev_word, _ = sentence[i-1]\n","        # TODO: assign the word vector for the previous word\n","        prev_word_vector = get_vector(prev_word)\n","      else:\n","        prev_word = \"FIRST_WORD\"\n","        prev_word_vector = np.zeros(NO_OF_DIMENSIONS)\n","\n","\n","\n","      # extract the next word (if it exists)\n","      if i < n_words - 1:\n","        next_word, _ = sentence[i+1]\n","        # TODO: assign the word vector for the next word\n","        next_word_vector = get_vector(next_word)\n","      else:\n","        next_word = \"LAST_WORD\"\n","        # TODO: assign a NO_OF_DIMENSIONS-sized vector with all 0s\n","        next_word_vector = np.zeros(NO_OF_DIMENSIONS)\n","\n","      # extract the suffix (last 3 characters)\n","      suffix = word[-3:]\n","\n","      # TODO: stack the word vectors for the previous, current, and next word\n","      # to create a 150-dimensional word vector and add it to the feature_vectors list\n","      combined_vector = np.hstack([prev_word_vector, word_vector, next_word_vector])\n","      feature_vectors.append(combined_vector)\n","\n","      # build the dictionary of indicator features\n","      ex_features = {\"prev\": prev_word,\n","                     \"next\":next_word,\n","                     \"word\":word,\n","                     \"suffix\": suffix\n","                     }\n","\n","      # add the dictionary of indicator features\n","      discrete_features.append(ex_features)\n","\n","  # construct the feature matrix that contains a row vector for each example\n","  feature_matrix = np.vstack(feature_vectors)\n","\n","  return feature_matrix, discrete_features, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"TFcTZrV7RqqU","colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"status":"ok","timestamp":1737623857878,"user_tz":0,"elapsed":251,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"c737120e-31b8-4e28-e14a-b4a6bcd4e4e5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["q3 results: All test cases passed!"],"text/html":["<p><strong><pre style='display: inline;'>q3</pre></strong> passed! 💯</p>"]},"metadata":{},"execution_count":43}],"source":["grader.check(\"q3\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"x8fcBHUSD80i"},"source":["### 4. Processing the training data\n","\n","We'll now build the feature matrix $X$ for the training data. As a first step, we'll get the matrix `train_vector_features` (let's call it $V$), the list of indicator features `train_discrete_features_raw`, and the list of labels `train_labels` using your `extract_vector_features` function.\n","\n","We'll then turn `train_discrete_features_raw` into a sparse matrix `train_discrete_features` (let's call it $D$) using a `DictVectorizer`."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"JX20XRpTadvB"},"outputs":[],"source":["# extract V, the list of indicator features, and the list of labels\n","train_vector_features, train_discrete_features_raw, train_labels = extract_vector_features(train_data_sentences)\n","\n","# compute D using a DictVectorizer\n","from sklearn.feature_extraction import DictVectorizer\n","vectoriser = DictVectorizer()\n","train_discrete_features = vectoriser.fit_transform(train_discrete_features_raw)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"a3grXAQeFNFv"},"source":["Once we've done that, we'll construct the matrix $X$ that we obtain by stacking the columns of $D$ and of $V$. Let's first check that this is possible by inspecting the shapes of these two matrices.\n","\n","If $N$ is the number of training examples, and $M$ is the number of unique features, then $D$ should have dimension $N\\times M$.\n","\n","$V$ should have the dimension $N\n","\\times 150$ since each feature vector is of dimension 150."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_sHpeUK7GDlU","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737627941875,"user_tz":0,"elapsed":225,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"8e4af84d-16e1-4842-ff55-bf0a77beb4e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dimension of D (train_discrete_features): (204579, 61425)\n","Dimension of V (train_vector_features): (204579, 300)\n"]}],"source":["shape_D = train_discrete_features.shape\n","shape_V = train_vector_features.shape\n","\n","print(f\"Dimension of D (train_discrete_features): {shape_D}\")\n","print(f\"Dimension of V (train_vector_features): {shape_V}\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"zeZp-KYYGgD4"},"source":["Once you've verified that the two matrices actually have the correct dimensions, we need to stack them. Usually you would use `np.hstack` or `np.vstack` for this. However, since the `DictVectorizer` returns a special sparse matrix, we'll have to use `scipy.sparse.hstack` or `scipy.sparse.vstack` for this. These functions work just like `np.hstack`/`np.vstack` but they can also handle sparse matrices.\n","\n","The resulting matrix should be of dimension $N \\times (M + 150)$.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxq2CtqFjOuo","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737627950780,"user_tz":0,"elapsed":4669,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"433107ae-b90b-4fd0-a125-814a82c3fdd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dimension of X (train_features_combined): (204579, 61725)\n"]}],"source":["import scipy\n","# N  = n_training_examples & M = n_unique_features, then  D  should have dimension  N×M .\n","# TODO: stack D and V to create X (train_features_combined)\n","train_features_combined = scipy.sparse.hstack([train_discrete_features, train_vector_features])\n","\n","\n","print(f\"Dimension of X (train_features_combined): {train_features_combined.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"CTrcx8mWRqqV","colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"status":"ok","timestamp":1737625643423,"user_tz":0,"elapsed":9110,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"2f7b1eba-ee6e-41bf-a71b-f2ed6507642c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["q4 results: All test cases passed!"],"text/html":["<p><strong><pre style='display: inline;'>q4</pre></strong> passed! 💯</p>"]},"metadata":{},"execution_count":51}],"source":["grader.check(\"q4\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"8aT3tKrRJr8p"},"source":["### 5. Training the classifier\n","\n","We are now ready to train the multinomial logistic regression classifier using the feature matrix $X$ (`train_features_combined`) that contains both the indicator features and the word vectors.\n","\n","We can do this as usual by defining a `LogisticRegression` classifier and then using it's `fit` method as shown in the following code block.\n","\n","```python\n","# train the classifier\n","from sklearn.linear_model import LogisticRegression\n","\n","# Intialise a new (multinomial) logistic classifier.\n","vector_classifier = LogisticRegression(max_iter=4000)\n","# Train the classifer\n","vector_classifier.fit(train_features_combined, train_labels)\n","```\n","\n","However, this classifier takes quite a bit of time to train (around 20-30 minutes), so instead of executing the code that is outlined here, you may want to load the result of training the classifier instead. You can do this by executing the next two cells.\n"]},{"cell_type":"code","source":["# train the classifier\n","from sklearn.linear_model import LogisticRegression\n","\n","# Intialise a new (multinomial) logistic classifier.\n","vector_classifier = LogisticRegression(max_iter=4000)\n","# Train the classifer\n","vector_classifier.fit(train_features_combined, train_labels)"],"metadata":{"id":"X3svBttGsnYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"WycGkdkmJrok","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737625647263,"user_tz":0,"elapsed":465,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"c38a923d-61fc-4107-8f26-4ae3dd8af3e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-01-23 09:47:26--  https://sebschu.com/files/plin0072/tutorials/tutorial2/vector_classifier.pkl\n","Resolving sebschu.com (sebschu.com)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n","Connecting to sebschu.com (sebschu.com)|185.199.108.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8375378 (8.0M) [application/octet-stream]\n","Saving to: ‘vector_classifier.pkl’\n","\n","\rvector_classifier.p   0%[                    ]       0  --.-KB/s               \rvector_classifier.p 100%[===================>]   7.99M  --.-KB/s    in 0.03s   \n","\n","2025-01-23 09:47:27 (284 MB/s) - ‘vector_classifier.pkl’ saved [8375378/8375378]\n","\n"]}],"source":["! cd data && wget https://sebschu.com/files/plin0072/tutorials/tutorial2/vector_classifier.pkl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rH4sj8i_K-X3","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737625664989,"user_tz":0,"elapsed":235,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"85f9db04-d371-4c87-842e-c77b8de70c27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully loaded the classifier from 'data/vector_classifier.pkl'.\n"]}],"source":["# this loads a pre-trained classifier instead of training a new one from scratch\n","\n","import pickle\n","with open(\"data/vector_classifier.pkl\", \"rb\") as c_f:\n","  vector_classifier = pickle.load(c_f)\n","\n","print(\"Successfully loaded the classifier from 'data/vector_classifier.pkl'.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"lTiAbs7zRqqV","colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"status":"ok","timestamp":1737625667242,"user_tz":0,"elapsed":204,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"252e7e3e-a83b-43c6-a9bc-7bd6131c0d0a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["q5 results: All test cases passed!"],"text/html":["<p><strong><pre style='display: inline;'>q5</pre></strong> passed! 🍀</p>"]},"metadata":{},"execution_count":54}],"source":["grader.check(\"q5\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"DDhUkNatMAF5"},"source":["### 6. Processing the evaluation data\n","\n","Next, we'll process the evaluation data and create a matrix $X$ out of the matrices $D$ and $V$, as we did for the training data.\n","\n","The following code already performs the feature extraction and constructs the matrices $D$ (`eval_discrete_features`) and $V$ (`eval_vector_features`) but you'll have to stack these two matrices to obtain the final feature matrix $X$. You again have to use `scipy.sparse.hstack` or `scipy.sparse.vstack` for this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Fsvv0MqdtlC","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737625864017,"user_tz":0,"elapsed":950,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"f3ad17a9-b66e-4420-f955-2de963054e77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dimension of X (eval_features_combined): (25149, 61575)\n"]}],"source":["# extract the word vectors, indicator features, and labels\n","eval_vector_features, eval_discrete_features_raw, eval_labels = extract_vector_features(eval_data_sentences)\n","eval_discrete_features = vectoriser.transform(eval_discrete_features_raw)\n","\n","# TODO: construct X\n","eval_features_combined = scipy.sparse.hstack([eval_discrete_features, eval_vector_features])\n","\n","print(f\"Dimension of X (eval_features_combined): {eval_features_combined.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"IjH46OsxRqqW","colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"status":"ok","timestamp":1737625872484,"user_tz":0,"elapsed":210,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"850cc85c-e604-4160-e8c1-99ccee92df90"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["q6 results: All test cases passed!"],"text/html":["<p><strong><pre style='display: inline;'>q6</pre></strong> passed! ✨</p>"]},"metadata":{},"execution_count":56}],"source":["grader.check(\"q6\")"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"swfaIrjGNquM"},"source":["### 7. Evaluating the classifier\n","\n","Finally, we'll use the `predict` method of the classifier with the feature matrix $X$ to obtain predictions on the evaluation data.\n","\n","We can then use this to compute the accuracy of the classifier on the evaluation data. Remember that the accuracy is defined as\n","\n","$$\\mbox{Accuracy} = \\sum_{i=1}^N \\frac{\\hat{y}_i == y_i}{N}$$\n","\n","You should see that the accuracy increased to 93.14%."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ik6BgnzGdoOk","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737626053947,"user_tz":0,"elapsed":228,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"c2e3a616-e65c-49e8-c3b6-65c08b6702da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on evaluation data: 93.14%\n"]}],"source":["# TODO: Compute the predictions of the new classsifier that also uses word vectors\n","eval_preds_vector = vector_classifier.predict(eval_features_combined)\n","\n","\n","correct = 0\n","n_examples = len(eval_preds_vector)\n","\n","# TODO: Compute the number of correct examples\n","for i in range(n_examples):\n","  if eval_labels[i] == eval_preds_vector[i]:\n","    correct += 1\n","\n","print(f\"Accuracy on evaluation data: {correct/n_examples * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"ZBd9VEy0RqqW","colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"status":"ok","timestamp":1737626056399,"user_tz":0,"elapsed":214,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"841696eb-ab3a-4d92-9a2d-a2959b5605b6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["q7 results: All test cases passed!"],"text/html":["<p><strong><pre style='display: inline;'>q7</pre></strong> passed! 🌈</p>"]},"metadata":{},"execution_count":58}],"source":["grader.check(\"q7\")"]}],"metadata":{"colab":{"provenance":[{"file_id":"1Lin0-OZbeYPkuD9i26CvUBlwoOVicbPG","timestamp":1737622051200}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"otter":{"assignment_name":"tutorial2"}},"nbformat":4,"nbformat_minor":0}