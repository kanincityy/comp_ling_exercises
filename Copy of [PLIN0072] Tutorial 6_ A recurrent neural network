{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Y9HU9KE_mUOcdMO1aXbGmOHlnfacdUzI","timestamp":1740646696805}],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Tutorial 6: A recurrent neural network language model\n","\n","In this tutorial, you'll implement a recurrent neural network language model together. We'll use a long-short term memory recurrent neural network (LSTM-RNN) that predicts one character at a time, and we'll train it on works by Shakespeare.\n","\n","This tutorial consists of two parts:\n","\n","1. Implementing the LSTM-RNN and the corresponding dataloader.\n","2. Implementing different sampling methods and comparing the outputs.\n","\n","Depending on how challenging you'd like this exercise to be, you can either implement part 1 yourself or you can copy and paste the solution to the LSTM from this notebook:\n","\n","[LSTM implementation notebook](https://colab.research.google.com/drive/1TghcZIlB8CO_x91nZiuxD_qOtI0ocW71)."],"metadata":{"id":"gUkrHXPmNZZF"}},{"cell_type":"markdown","source":["## Part 1: Implementing the character LSTM RNN\n","\n","To implement this model, we'll first have to prepare the training data. As mentioned above, we'll train the model on a collection of works by Shakespeare.\n","\n","Run the following cell to download the data to a file called `data/shakespeare_input.txt`."],"metadata":{"id":"bq4U7Nwp3Ar7"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"G-wTHRfeMrna","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740668733909,"user_tz":0,"elapsed":1104,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"0ce4bb60-53f1-4b22-f7fe-fb160164f916"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-02-27 15:05:33--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n","Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n","Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4573338 (4.4M) [text/plain]\n","Saving to: â€˜shakespeare_input.txtâ€™\n","\n","shakespeare_input.t 100%[===================>]   4.36M  5.54MB/s    in 0.8s    \n","\n","2025-02-27 15:05:34 (5.54 MB/s) - â€˜shakespeare_input.txtâ€™ saved [4573338/4573338]\n","\n"]}],"source":["# Download data\n","!mkdir -p data && cd data && wget https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt"]},{"cell_type":"markdown","source":["### Implementing the dataset class\n","\n","Next, we'll have to implement a `Dataset` class. For training an RNN, we need to turn the text file into a list of contexts and tokens to predict. Here we are training a character-based model, that is, a model that predicts the next character given all the previous characters.\n","\n","If we assume that we have a mini text of the form \"The cat slept.\", this would produce the following examples:\n","\n","```python\n","[(['T'], 'h'),\n","(['T', 'h'], 'e'),\n","(['T', 'h', 'e'], ' '),\n","(['T', 'h', 'e', ' '], 'c'),\n","(['T', 'h', 'e', ' ', 'c'], 'a'),\n","(['T', 'h', 'e', ' ', 'c', 'a'], 't'),\n","(['T', 'h', 'e', ' ', 'c', 'a', 't'], ' '),\n","(['T', 'h', 'e', ' ', 'c', 'a', 't', ' '], 's'),\n","(['T', 'h', 'e', ' ', 'c', 'a', 't', ' ', 's'], 'l'),\n","(['T', 'h', 'e', ' ', 'c', 'a', 't', ' ', 's', 'l'], 'e'),\n","(['T', 'h', 'e', ' ', 'c', 'a', 't', ' ', 's', 'l', 'e'], 'p'),\n","(['T', 'h', 'e', ' ', 'c', 'a', 't', ' ', 's', 'l', 'e', 'p'], 't'),\n","(['T', 'h', 'e', ' ', 'c', 'a', 't', ' ', 's', 'l', 'e', 'p', 't'], '.')]\n","```\n","\n","**Padding and truncating**: In practice, we can train the network much faster, if the number of characters in the context for each example in the batch is the same. Furthermore, as we've discussed in class, most of the information in RNNs that is very far away from the token it tries to predict is not well represented in the hidden state anyways. For these reasons, we usually a) **truncate each example at a fixed context length** and b) **add special `<PAD>` (padding) tokens** to the beginning of the context if it is shorter than the context length.\n","\n","If we assume a context length of 4, then the examples for the mini-text \"The cat slept.\" would look follows:\n","\n","```python\n","[(['<PAD>', '<PAD>', '<PAD>', '<PAD>'], 'T'),\n","(['<PAD>', '<PAD>', '<PAD>', 'T'], 'h'),\n","(['<PAD>', '<PAD>', 'T', 'h'], 'e'),\n","(['<PAD>', 'T', 'h', 'e'], ' '),\n","(['T', 'h', 'e', ' '], 'c'),\n","(['h', 'e', ' ', 'c'], 'a'),\n","(['e', ' ', 'c', 'a'], 't'),\n","([' ', 'c', 'a', 't'], ' '),\n","(['c', 'a', 't', ' '], 's'),\n","(['a', 't', ' ', 's'], 'l'),\n","(['t', ' ', 's', 'l'], 'e'),\n","([' ', 's', 'l', 'e'], 'p'),\n","(['s', 'l', 'e', 'p'], 't'),\n","(['l', 'e', 'p', 't'], '.')]\n","```"],"metadata":{"id":"5t93VWdRE0Uc"}},{"cell_type":"markdown","source":["Your job now is to finish implementing the class `TextCharacterDataset`.\n","\n","\n","The method `load_text_file` should do the following things:\n","\n","1. Read in the text file located at `self.text_file_path`.\n","2. Using this text file, build up a dictionary `self.char2index` that contains an entry for each unique character that is present in the text file and maps it to a unique integer. The first entry, as already implemented, should be for the special `<PAD>` token with an index of `0`.\n","3. Using this text file, reate a list `self.characters` that contains all **indices** of characters that are present in the text file.\n","\n","**Hint**: You can build up both of these data structures in parallel, so you only have to go through the text file once.\n","\n","For example, if the contents of the text file were `\"The cat slept.\\n\"`, then `load_text_file` should populate `self.characters` and `self.char2index` as follows:\n","\n","```\n","self.char2index = {\n","  \"<PAD>\": 0,\n","  \"T\": 1,\n","  \"h\": 2,\n","  \"e\": 3,\n","  \" \": 4,\n","  \"c\": 5,\n","  \"a\": 6,\n","  \"t\": 7,\n","  \"s\": 8,\n","  \"l\": 9,\n","  \"p\": 10,\n","  \".\": 11,\n","  \"\\n\": 12\n","}\n","\n","self.characters = [\n","  1,\n","  2,\n","  3,\n","  4,\n","  5,\n","  6,\n","  7,\n","  4,\n","  8,\n","  9,\n","  3,\n","  10,\n","  7,\n","  11,\n","  12\n","]\n","\n","```\n"],"metadata":{"id":"yfX-nUi2JmvB"}},{"cell_type":"markdown","source":["The `__getitem__` method should then build the context for the `index`-th character of length `self.context_size` using the characters in `self.characters`. In other words, it should return the `self.contex_size` characters before the `index`-th character as the context, and the `index`-th character as he target. Both the context and the characters should be represented by the indices."],"metadata":{"id":"_wdcvs78OZQo"}},{"cell_type":"code","source":["import torch\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","class TextCharacterDataset(Dataset):\n","\n","    def __init__(self, text_file_path, context_size):\n","        # store the path to the text file\n","        self.text_file_path = text_file_path\n","        # set the context size\n","        self.context_size = context_size\n","        # call load_text_file to populate self.char2index and self.characters\n","        self.load_text_file()\n","\n","\n","    def load_text_file(self):\n","        self.char2index = {\"<PAD>\": 0}\n","        self.characters = []\n","        # TODO: finish implementing this method\n","        # Read the file\n","        with open(self.text_file_path, \"r\", encoding=\"utf-8\") as f:\n","            text = f.read()\n","\n","        # Process characters in one pass\n","        next_index = 1  # Start indexing from 1 (since 0 is for <PAD>)\n","        for char in text:\n","            # If char is new, add to dictionary\n","            if char not in self.char2index:\n","                self.char2index[char] = next_index\n","                next_index += 1\n","            # Store character index\n","            self.characters.append(self.char2index[char])\n","\n","    def __len__(self):\n","        # we'll have one example for each character\n","        return len(self.characters)\n","\n","    def __getitem__(self, index):\n","        # TODO: set this to the id of the to be predicted character, i.e., the\n","        # Build the context list (self.context_size characters before index)\n","        start_idx = max(0, index - self.context_size)\n","        context = self.characters[start_idx:index]\n","\n","        # Pad context if it's too short\n","        if len(context) < self.context_size:\n","            context = [0] * (self.context_size - len(context)) + context  # Pad with \"<PAD>\" token (index 0)\n","\n","        target = self.characters[index]  # Target is the current character\n","\n","\n","        # convert the target and the context to LongTensors\n","        # so that they can be used as input to the model\n","        input_ids = torch.LongTensor(context)\n","        target_id = torch.LongTensor([target])\n","\n","        return input_ids, target_id\n","\n"],"metadata":{"id":"TGlPuH68Ovfa","executionInfo":{"status":"ok","timestamp":1740668741675,"user_tz":0,"elapsed":3540,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["train_data = TextCharacterDataset(\"data/shakespeare_input.txt\", context_size=50)"],"metadata":{"id":"lftEiqgNSbah","executionInfo":{"status":"ok","timestamp":1740668750980,"user_tz":0,"elapsed":654,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["If you've implemented this function correctly, the following cell should output\n","```python\n","(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0]),\n"," tensor([1]))\n"," (tensor([ 6,  4, 17,  9, 20, 26, 27, 12, 12,  1,  2,  3,  4,  5,  6,  7,  2,  5,\n","          2,  8,  9, 10, 11, 12, 31, 15, 22,  6, 20,  3,  9,  6, 20, 29, 29,  6,\n","          3,  9,  4, 15, 29, 32,  9, 19,  6,  3, 20,  5, 23,  9]),\n"," tensor([3]))\n","```"],"metadata":{"id":"byMSrZh3RDdA"}},{"cell_type":"code","source":["print(train_data[0])\n","print(train_data[123])"],"metadata":{"id":"C63ORS4WRCnK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740668785846,"user_tz":0,"elapsed":51,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"1df3b473-1f3e-4ac9-db49-7de9f293d0ed"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0]), tensor([1]))\n","(tensor([ 6,  4, 17,  9, 20, 26, 27, 12, 12,  1,  2,  3,  4,  5,  6,  7,  2,  5,\n","         2,  8,  9, 10, 11, 12, 31, 15, 22,  6, 20,  3,  9,  6, 20, 29, 29,  6,\n","         3,  9,  4, 15, 29, 32,  9, 19,  6,  3, 20,  5, 23,  9]), tensor([3]))\n"]}]},{"cell_type":"markdown","source":["### Implementing the model\n","\n","For the actual model, we'll implement a recurrent neural network (RNN) that we've covered in Week 4 (look back at the slides for how they work at a conceptual level). However, instead of using a vanilla RNN, we'll use the more sophisticated long-short term memory (LSTM) network, which tends to learn much better representations than the standard RNN.\n","\n","The equations for an LSTM are a bit more complicated (see the [LSTM Wikipedia page](https://en.wikipedia.org/wiki/Long_short-term_memory) or [Chapter 8.5 of Jurafsky and Martin (2025)](https://web.stanford.edu/~jurafsky/slp3/8.pdf)) but fortunately, there is already an existing implementation of the LSTM in PyTorch.\n","\n","First, take a look at the Pytorch documentation for the `nn.LSTM` class: [nn.LSTM documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html).\n","\n","This module is initialised with an `input_size`, that is the dimension of the embeddings, and a `hidden dimension`, that is the dimension of the hidden vectors.\n","\n","We'll organise our data such that the first dimension of the network are the batches, so we'll have to set `batch_first=True` (talk to any of the teaching staff if you'd like to learn more about what this means).\n","\n","As always with neural network implementations, you'll have to implement the `__init__` method and the `forward` method.\n","\n","The `__init__` method should set up all the weight matrices we need:\n","* an embedding matrix that maps each of the `n_character` characters to an embedding of dimension `embedding_dimension`  (use `nn.Embedding` to define this matrix).\n","* the weights for the LSTM cells (use `nn.LSTM` here).\n","* an output layer that takes the hidden representation of the last LSTM state and computes the logits that could be passed through a softmax layer.\n","\n","The `forward` method takes both the last token to make a prediction and a hidden state from the previous computations and passes it through the LSTM. It then returns the logits and the hidden state. Note that if no hidden state is given to the model, it will use a vector of all 0s as the hidden state. We usually do this for the first character when sampling from the model.\n","\n","For efficiency reasons, the LSTM can actually take in a sequence of tokens, in which case it performs the recurrence itself. This means when we train the model, we do not have to input the context tokens one-by-one but instead, we can pass all the tokens as a list and the implementation automatically computes the recurrence."],"metadata":{"id":"35qheWBKZWGT"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class CharLSTMModel(nn.Module):\n","\n","    def __init__(self, n_characters, embedding_dimension, hidden_dimension):\n","        super().__init__()\n","        self.hidden_dimension = hidden_dimension\n","\n","        # TODO: define the embedding layer here\n","        self.embedding_layer = nn.Embedding(n_characters, embedding_dimension)\n","\n","        # TODO: define the LSTM cell here, using the parameters specified above\n","        # batch_first â€“ If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature)\n","        # Because input is batch from our data not just single characters (seq).\n","        # Cannot apply dropout to a single layer LSTM - only in between LSTM layers!!!\n","        # Dropout documentation: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n","\n","        self.LSTM = nn.LSTM(embedding_dimension, hidden_dimension, num_layers=1, batch_first=True)\n","\n","        # TODO: define the output layer here\n","        # Takes the hidden representation of the last LSTM state and computes the logits that could be passed through a softmax layer\n","        self.output_layer = nn.Linear(hidden_dimension, n_characters)\n","\n","    def forward(self, x, hidden):\n","\n","        # This function needs to do the following computations\n","        # e = embed x using the embedding layer\n","        e = self.embedding_layer(x)\n","\n","        # lstm_out, new_hidden = passing e and hidden (if it is not None) through\n","        # the LSTM\n","        if hidden is not None:\n","          lstm_out, new_hidden = self.LSTM(e, hidden)\n","        else:\n","          lstm_out, new_hidden = self.LSTM(e)\n","\n","        # this extracts the last LSTM hidden represenation in case we input multiple tokens into\n","        # the LSTM\n","        lstm_out = lstm_out[:, -1, :]\n","\n","        # logits = passing the last LSTM hidden representation through the output layer\n","        logits = self.output_layer(lstm_out)\n","        return logits, new_hidden\n","\n","\n"],"metadata":{"id":"mfxDe3iJiTFX","executionInfo":{"status":"ok","timestamp":1740668788762,"user_tz":0,"elapsed":6,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### Training the network\n","\n","The following code implements the training procedure. We'll use a batch size of 1024, a hidden dimension of 128, and an embedding size of 128 here.\n"],"metadata":{"id":"3bbhvOAyXqs8"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","import tqdm\n","\n","# The following few lines check whether a GPU is available, and if so,\n","# they run everything on a GPU which will be much faster.\n","device = \"cpu\"\n","if torch.cuda.is_available():\n","  device = \"cuda\"\n","print(f\"Using {device} device\")\n","\n","# Let's define the batch_size, the window size, the output size, and the size of the hidden layer\n","batch_size = 1024\n","n_characters = len(train_data.char2index)\n","hidden_size = 128\n","embedding_size = 128\n","\n","\n","# Let's define the model and move it to the GPU, if available.\n","model = CharLSTMModel(n_characters, embedding_size, hidden_size).to(device)\n","# We use a cross-entropy loss here\n","loss_fn = nn.CrossEntropyLoss().to(device)\n","# Finally, let's define the optimiser\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","# This loads the data for training\n","train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","\n","# The function for running model training\n","def train(model, loss_fn, optimizer, dataloader, epochs=150):\n","    # number of times we go through the training data\n","    # set the mode of the model to training so that parameters are being updated\n","    model.train()\n","\n","    for epoch in range(epochs):\n","\n","        # intialise the loss at the beginning of each epoch\n","        epoch_losses = []\n","\n","        # iterate through all training batches\n","        for X, y in tqdm.tqdm(dataloader):\n","\n","            seq_len = X.size(1)\n","            batch_len = X.size(0)\n","\n","            # move the input and the labels to the GPU, if we are using a GPU\n","            X = X.to(device)\n","            y = y.to(device).flatten()\n","\n","            # Initialise the hidden representation (this is h0)\n","            h = None\n","\n","            # reset the optimiser\n","            optimizer.zero_grad()\n","\n","            # make predictions using the model\n","            logits, h = model(X, h)\n","\n","            # compute the loss for the current predictions\n","            loss = loss_fn(logits, y)\n","            # perform backpropagation\n","            loss.backward()\n","            # and update the weights\n","            optimizer.step()\n","\n","            # add the loss of the current batch to the loss of the epoch\n","            epoch_losses.append(loss.item())\n","\n","            if len(epoch_losses) % 1000 == 0:\n","                print(\"\\nEpoch: {0}, current loss: {1}, \".format(epoch, sum(epoch_losses)/len(epoch_losses)))\n","\n","\n","        # print the loss at the end of every epoch\n","        print(\"\\nEpoch: {0}, final loss: {1}, \".format(epoch, sum(epoch_losses)/len(epoch_losses)))\n","\n","\n","\n","\n"],"metadata":{"id":"MPfgJjIEjT7r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740668796341,"user_tz":0,"elapsed":5285,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"ddd35fb6-a664-424c-bef3-fce3208b6608"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda device\n"]}]},{"cell_type":"markdown","source":["Run the following cell to train the model for one epoch."],"metadata":{"id":"n5pK_qkQY875"}},{"cell_type":"code","source":["train(model, loss_fn, optimizer, train_dataloader, epochs=1)"],"metadata":{"id":"jCRpMyX-kP4h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740649634164,"user_tz":0,"elapsed":174936,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"052ed193-cf1c-423a-8c76-50eae3cba259"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 23%|â–ˆâ–ˆâ–Ž       | 1006/4467 [00:39<01:56, 29.69it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0, current loss: 2.1226634149551393, \n"]},{"output_type":"stream","name":"stderr","text":[" 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2003/4467 [01:18<01:31, 27.04it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0, current loss: 1.9429803636074066, \n"]},{"output_type":"stream","name":"stderr","text":[" 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 3006/4467 [01:56<00:49, 29.53it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0, current loss: 1.8499480499426524, \n"]},{"output_type":"stream","name":"stderr","text":[" 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 4002/4467 [02:35<00:20, 23.04it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0, current loss: 1.7897722911834717, \n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4467/4467 [02:54<00:00, 25.55it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch: 0, final loss: 1.7684113741294634, \n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["Training the model for only one epoch will not result in very good performance. I've already trained the model for 50 epoch and you can download my weights by running the following two cells."],"metadata":{"id":"4StDh9r7ZLzR"}},{"cell_type":"code","source":["# Download model\n","!cd data && wget http://sebschu.com/files/plin0072/tutorials/tutorial6/model.p"],"metadata":{"id":"9ViT_HzLaYBn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740668775286,"user_tz":0,"elapsed":604,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"8563b970-a694-47e6-c5e3-7f2077457946"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-02-27 15:06:15--  http://sebschu.com/files/plin0072/tutorials/tutorial6/model.p\n","Resolving sebschu.com (sebschu.com)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n","Connecting to sebschu.com (sebschu.com)|185.199.108.153|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://sebschu.com/files/plin0072/tutorials/tutorial6/model.p [following]\n","--2025-02-27 15:06:15--  https://sebschu.com/files/plin0072/tutorials/tutorial6/model.p\n","Connecting to sebschu.com (sebschu.com)|185.199.108.153|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 602112 (588K) [text/x-pascal]\n","Saving to: â€˜model.pâ€™\n","\n","model.p             100%[===================>] 588.00K  --.-KB/s    in 0.03s   \n","\n","2025-02-27 15:06:15 (19.9 MB/s) - â€˜model.pâ€™ saved [602112/602112]\n","\n"]}]},{"cell_type":"code","source":["# Torch has a load function which allows you to load a saved model\n","# f = file-like object\n","# map_location = expects a a function, torch.device, string or a dict specifying how to remap storage locations\n","model = torch.load(\"data/model.p\", map_location=torch.device(device))"],"metadata":{"id":"6Kb8gS10C0Fs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740668799530,"user_tz":0,"elapsed":21,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"faf626ae-ae2e-4578-ea0c-cd74b8d33db5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-81fa2a7c00b3>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(\"data/model.p\", map_location=torch.device(device))\n"]}]},{"cell_type":"markdown","source":["Congratulations, you've now implementd and loaded a recurrent neural network! ðŸŽ‰"],"metadata":{"id":"fSNMFM28bmQC"}},{"cell_type":"markdown","source":["### Part 2: Implementing different sampling methods\n","\n","For the second part of the Tutorial, let's implement some sampling methods that we discussed in this week's lecture.\n","\n","Let's first define some utility functions for printing the output."],"metadata":{"id":"0h-y1kIsbrkM"}},{"cell_type":"code","source":["# This defines a index2chars dictionary that lets us translate\n","# character indices back to their characters\n","# this is the reverse dictionary from train_data.chars2index\n","index2chars = {}\n","# iterate over character, index in our dictionary items\n","for c, i in train_data.char2index.items():\n","    # add new entry of character for each index\n","    index2chars[i] = c\n","\n","# This is a utility method for printing out the text represented by\n","# a list of indices\n","def print_output_indices(output_indices):\n","    # initialise list of chars\n","    chars = []\n","    # iterate over index from output_indices\n","    for idx in output_indices:\n","        # append character value from our index2chars dictionary\n","        chars.append(index2chars[idx])\n","\n","    # join() method takes all items in an iterable and joins them into one string\n","    print(\"\".join(chars))"],"metadata":{"id":"HTsBSA42rP8d","executionInfo":{"status":"ok","timestamp":1740668801819,"user_tz":0,"elapsed":3,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["In the following cell, I've already implemented greedy decoding."],"metadata":{"id":"UE13-QYvc7Ta"}},{"cell_type":"code","source":["# Greedy decoding\n","# Always choose word with highest prob.\n","\n","# The context that we prompt the model with:\n","prompt = \"JULIET:\"\n","\n","# translate the prompt into indices\n","# iterate over our training data using the char2index dictionary\n","# identify the character that matches our prompt\n","# add this to a list of output_indices\n","output_indices = [train_data.char2index[c] for c in prompt]\n","\n","# turn the list into a PyTorch tensor and move it to the GPU, if available\n","# Multi-dimensional matrix containing elements of a single data type, specifically 64-bit signed integers.\n","# This tensor type is often used for indexing and other operations that require integer values.\n","prev_chars = torch.LongTensor([output_indices]).to(device)\n","\n","# Initialise the hidden state\n","hidden = None\n","\n","# how many characters to generate\n","# capping at specific value because we didn't implement a stop token\n","output_len = 5000\n","\n","# set the model to evaluation mode\n","model.eval()\n","# iterate over values in output_len\n","for i in range(output_len):\n","\n","    # compute the logits for the next character\n","    logits, hidden = model(prev_chars, hidden)\n","\n","    # turn them into probabilities\n","    probs = F.softmax(logits, dim=-1)\n","\n","    # get the highest scoring character\n","    #.view(1,1) turns the tensor into a 2D tensor, which is what model expects as input\n","    prev_chars = torch.argmax(probs).view(1,1)\n","\n","    # add the character to the list\n","    # we'll have to move it back to the CPU, if we are using a GPU and convert it from\n","    # the pytorch format to a regular integer\n","    output_indices.append(prev_chars.cpu().view(1).tolist()[0])\n","\n","\n","# Let's print the generation:\n","print_output_indices(output_indices)\n"],"metadata":{"id":"jpfEenbYSW-z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740668806366,"user_tz":0,"elapsed":2944,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"681ac265-249f-4e07-969a-499ab84cfdee"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["JULIET:\n","The friends, I will not see thee to the contents\n","And shame the sea and the sea and the sea and the sea and the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea,\n","And then the sense and such a strange that we may be so far of the sea and the sea, and the sea,\n","And then the sense and such a strange that we may be so far of the s\n"]}]},{"cell_type":"markdown","source":["As you can see, the greedy decoding output is not very good and very repetitive.\n","\n","Instead, let's implement some sampling based methods.\n","\n","First, implement random sampling. You can adapt the code from greedy sampling and use the [`torch.multinomial`](https://pytorch.org/docs/stable/generated/torch.multinomial.html) function that samples from a distribution.\n","\n","The output should already look a lot better and less repetitive."],"metadata":{"id":"OtNbjzTmesZG"}},{"cell_type":"code","source":["# TODO: Implement random sampling decoding here\n","# Randomly select word from probability distribution\n","# Problem = many odd, low-probability words in the tail of the distribution (any word can be sampled!)\n","\n","# The context that we prompt the model with:\n","prompt = \"JULIET:\"\n","\n","# translate the prompt into indices\n","# iterate over our training data using the char2index dictionary\n","# identify the character that matches our prompt\n","# add this to a list of output_indices\n","output_indices = [train_data.char2index[c] for c in prompt]\n","\n","# turn the list into a PyTorch tensor and move it to the GPU, if available\n","# Multi-dimensional matrix containing elements of a single data type, specifically 64-bit signed integers.\n","# This tensor type is often used for indexing and other operations that require integer values.\n","prev_chars = torch.LongTensor([output_indices]).to(device)\n","\n","# Initialise the hidden state\n","hidden = None\n","\n","# how many characters to generate\n","# capping at specific value because we didn't implement a stop token\n","output_len = 5000\n","\n","# set the model to evaluation mode\n","model.eval()\n","# iterate over values in output_len\n","for i in range(output_len):\n","\n","    # compute the logits for the next character\n","    logits, hidden = model(prev_chars, hidden)\n","\n","    # turn them into probabilities\n","    probs = F.softmax(logits, dim=-1)\n","\n","    # torch.multinomial = Returns a tensor where each row contains num_samples indices sampled from the multinomial\n","    prev_chars = torch.multinomial(probs, 1).view(1,1)\n","\n","    # add the character to the list\n","    # we'll have to move it back to the CPU, if we are using a GPU and convert it from\n","    # the pytorch format to a regular integer\n","    output_indices.append(prev_chars.cpu().view(1).tolist()[0])\n","\n","\n","# Let's print the generation:\n","print_output_indices(output_indices)\n"],"metadata":{"id":"aFSj5axdlnaV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740668824975,"user_tz":0,"elapsed":3587,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"outputId":"725099c8-8fc0-44a0-c9b3-19d6e2f9ffce"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["JULIET:\n","Why love thee,\n","In the earth upon you, I would in thy gieformain ones and forest, Versarishs:\n","Chire, me so near my life, protector from an one, yee, sir, I your cruithe?\n","\n","SIR TOBY BELCH:\n","All pooteful for daughterfed.\n","\n","CORIOLANUS:\n","Dannicisf evieed that every coped, what, and troub?\n","\n","HOLOFERNES:\n","But sure it is too much sore contemply.\n","\n","ROMEO:\n","Not it, the old for life a word: as night gracious wounds,\n","And on beets indeed, and won them be due; and did,--\n","\n","KING OF FRANCE:\n","Old her worsips o' thee, like it, away! lose upon Venus over to, it said.\n","Is more\n","Which is married by some misear to me into a things of furst\n","What speak more from Entaffelged envose and your sword; and his charged my dronago,\n","When we then suclon hell of subject and boy defent this gastily.\n","\n","SHYMOCKu:\n","What years here sins and would tilld 'twas show'd my dover? preserve the lines of a good desires not great practies,\n","If webservol have mercy from me: 'tis not not too blood for their then.\n","\n","MISTRESS QUICKLY:\n","That field.\n","What partly? I take him foot bound him; here in Saint of your master's asselves, my earth, I bedewshiment,\n","Do werl about challenger:\n","For my will mischise a difference as offence, to Jokn's Herefords of a sit\n","Common these suspicion, for the fires are death. Be see't by thee. What's this it constatten! the son. Go, touch 'Moces majestis, a mass, I in advars' nor occasion on ensue,\n","My dreaming, thy lord then knew the queen's mind of venip this gentlemer invider and for our Venan, and sir, the chopts the worsed\n","Will you know you have the king's instructly is ire my mourners,\n","Nerced the lines if this want of his best and sign child, I must lead my father's mocksomething,\n","When singlence of my lady o' the rop my sches wornds.\n","\n","KING LEWIS XI:\n","Tress thyself\n","To scorn cheek and heaven of my core of the kind is very once,\n","That her embstagidings since, we do not? What pauch in assexle remumenties thou hast I with yourself or fledged,\n","That is the most for baseness; whom we are your fancy; we will not.\n","\n","HERMIONE:\n","Thou dost more too fursty grief doth himself than so oftens.\n","Thy things?\n","\n","PAROLLES:\n","Now is to begin too near his drumf;\n","But I confite my lady--\n","They shall done these worthy words.\n","\n","LONGOUSE:\n","Madam, with the health makes you, I saw your courses o' the force's emestroked rues, but then woo do, is the gentleman,\n","Now, by your duckless who contains and potence. What was they come? is the death is bornfire, I'll not rather desernity, attendant.\n","Who do you thy cock,\n","Not ancient murder; whither four boke, toward thee,\n","With appleft sucretient sure doth all wound exacts you, take me: tell me: now all this I will discover; or is ever happine\n","Shall breaky one fool of these, take frees of my standing, anger;\n","You see you this man condemnation; you'll die; or slanded, do say you, gentle Caesar store not.\n","Go too much.\n","\n","DUCHESS OF YORK:\n","What 'twere mettle would be join? What and do bid lords end their mistoon of\n","Mars, your longer of our survey!\n","There is no more. I'll on her proforts; and nothing is torgle Cassius cheer-kiss.\n","\n","TIMON:\n","I love your words fiends by worse, tell him 'goot,' approve yourself our streets of reportry:\n","Shall every Romeo's dogs; and doth not\n","As most usianors.\n","Alas, sister is in him that I could much our glory sketoman;\n","In the love would such offices and friends, that would mean there's a never in his convould of our sort--\n","For she is not utter, and lose away thy fells, i' like the greein.\n","\n","Sit may whomainseather, fight? ho, my liege, Troilus.\n","\n","VIOLLA:\n","O come to-night?\n","\n","ROSALINE:\n","So wither'd, believe thee!\n","\n","PETRUCHIO:\n","Muster so of that is the shipp'd by his cheeks, than it fain at;\n","For your brother's youth is no words, I latsen for yourself, the corrablined; a dumb burden it wonly defarity,\n","Where I stay to disquiet: I in the yondship that I warrant you, my rights,\n","Before, painted me to one your dispecion, and doubt not theren's house of Englander, show lords.\n","\n","ANTONY OF:\n","Why there did smatter addile; and there did lie the\n","Have tale I had night; and so he was not our company!\n","\n","LUCETTA:\n","Faith, fling no ragian in the story do ye.\n","\n","GRATIL:\n","He now, fair fac his pound Pyra femele virtuous purse, boy.\n","\n","PRINCE HENRY:\n","With his fingers of him, my lord?\n","\n","MARIA:\n","Be she makes my son shall audient but lender; wenly followed?\n","But he'll heels. What more? My lord words is some-starm away, bid,\n","Ere I see him not at arms--as that Henry's Duke of Hal say name as\n","It earth lie so it being my handstaff,\n","Ay, as well-something by curses; nect the corry, by white passions your overchard is defust to perform' and I think; for\n","rather forth an amoral sons for, having gong pains on the war, by soldiers;\n","Fall it say the loss. Then any faults it between your bounty of me.\n","\n","PROSPERO:\n","I hate thee the objection, by this so much to brain\n","Our answixt up out.\n","\n","TIMON:\n","We makes deny none.\n","\n","VIOLA:\n","Any houses, Antony's overthorgiter bless'd with their pesses!\n","Wilt his father's letjowerh proport tender bold and wax it,--throw it now, reep her sorry,\n","Or and Jamonss the time I \n"]}]},{"cell_type":"markdown","source":["Next, implement top-k sampling. To help you get started, I impplemented a function that takes in logits and modifies them so that all tokens that are not in the top-k set end up with logits of -$\\infty$, which results in a probability of 0 when it is passed through the softmax.\n","\n","Use this function to implement top-k sampling.\n","\n","Play around with different settings of $k$. How does it affect the generations? Does this seem to work better than random sampling?"],"metadata":{"id":"SHl4dF4LfUBV"}},{"cell_type":"code","source":["# Top-k has a k number of words, which is fixed\n","# Compute likelihood of each word in V, sort them by likelihood keeping only k most likely words\n","# Renormalise scores of k words\n","# Randomly sample a word from within k most prob. words\n","\n","def top_k_filtering(logits, top_k):\n","    \"\"\" Filter a distribution of logits using top-k filtering\n","        Args:\n","            logits: logits distribution shape (vocabulary size)\n","            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n","    \"\"\"\n","    logits = logits.view(-1)\n","    # Remove all tokens with a probability less than the last token of the top-k\n","    indices_to_remove = logits < torch.topk(logits, top_k)[0][-1]\n","    logits[indices_to_remove] = -float('Inf')\n","\n","    return logits"],"metadata":{"id":"5XuORjBrGSRj","executionInfo":{"status":"ok","timestamp":1740668844297,"user_tz":0,"elapsed":2,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# TODO: implement top-k decoding here\n","# The context that we prompt the model with:\n","prompt = \"JULIET:\"\n","\n","# translate the prompt into indices\n","# iterate over our training data using the char2index dictionary\n","# identify the character that matches our prompt\n","# add this to a list of output_indices\n","output_indices = [train_data.char2index[c] for c in prompt]\n","\n","# turn the list into a PyTorch tensor and move it to the GPU, if available\n","# Multi-dimensional matrix containing elements of a single data type, specifically 64-bit signed integers.\n","# This tensor type is often used for indexing and other operations that require integer values.\n","prev_chars = torch.LongTensor([output_indices]).to(device)\n","\n","# Initialise the hidden state\n","hidden = None\n","\n","# how many characters to generate\n","# capping at specific value because we didn't implement a stop token\n","output_len = 5000\n","\n","# set the model to evaluation mode\n","model.eval()\n","# iterate over values in output_len\n","for i in range(output_len):\n","\n","  # compute the logits for the next character\n","  logits, hidden = model(prev_chars, hidden)\n","\n","  logits = top_k_filtering(logits, top_k= 12)\n","\n","  # turn them into probabilities\n","  probs = F.softmax(logits, dim=-1)\n","\n","  # torch.multinomial = Returns a tensor where each row contains num_samples indices sampled from the multinomial\n","  prev_chars = torch.multinomial(probs, 1).view(1,1)\n","\n","  # add the character to the list\n","  # we'll have to move it back to the CPU, if we are using a GPU and convert it from\n","  # the pytorch format to a regular integer\n","  output_indices.append(prev_chars.cpu().view(1).tolist()[0])\n","\n","# Let's print the generation:\n","print_output_indices(output_indices)"],"metadata":{"id":"hPzXE8XaCGVv","executionInfo":{"status":"ok","timestamp":1740669764704,"user_tz":0,"elapsed":3568,"user":{"displayName":"Tatiana Limonova","userId":"12959421281268165602"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b888b404-f28f-4d03-abc2-c5b66dda2eae"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["JULIET:\n","Sir Crign thou captsiman?\n","\n","KING CLAUDIUS:\n","O thus! the saying-speaks hardly nothing, sir, by miel,\n","A curticle.\n","\n","LORENZO:\n","What mean thee o' that the babes! I begins she disdalling:\n","Who is it the breast, and strike us, by your son.\n","\n","LUCIUS:\n","There's said.\n","\n","CRESSIDA:\n","Master, but I cannot sent me? thy lips say.\n","\n","CADE:\n","Master Bloof, I may give our commission!\n","\n","MORTIME:\n","Here, to we shake my mouth;\n","As in his companion of warrate me a wished fast, what is with him! Would thou loss these strup?\n","And, for in that you live me he weeps, me in your course hath never proved my blood,\n","Which of our whose spoin his blounded hither swoon my crustusing wemper'd but like on.\n","\n","DESDEMONA:\n","Now-swere you to prune once not.\n","For learning sovereign I will news moder to send your empress,\n","Are, brother and the bleed beggar in a possession,\n","I do borne, and the deam.\n","\n","Cabord:\n","Tell it insurbert you seal you this falsely and like weed.\n","\n","HASTINGS:\n","What! I could come; she is the best age: he had not followed it: when, where you, further\n","Wit with his master maw,\n","I wish that thougg are time she saw it to me.\n","\n","FERDINAND:\n","O days that is cheeks and sight.\n","\n","KING RICHARD III:\n","I'll show your maritainer a that is\n","To wash'd witness farewell; I must be no sons, he basely down, in the shore both thou all that they, the more, the same pray, what's the love and cure, I'll be court and summer of him.\n","\n","Lord Crenerse:\n","What, at the canitile?\n","\n","Second Gentleman:\n","Marry figeable lady as you.\n","\n","DUMANTIS:\n","Then wagage?\n","\n","AROLLE:\n","Nay, I'll not mad, find me, an attempt against the brathers.\n","\n","PICH:\n","These wisdom shorten's majesty, or war this letterforms, for speed; and take us without as\n","Against your hill-flying:\n","How comes so, I pretter their father's cook:\n","If thou hast a disclaim, his monument for their wife. If it, I have succeesanion have not weals:\n","Here is a pestard worth of this spirit-like him.\n","\n","DUKE:\n","Your losh and accused is buckle.\n","\n","DULL:\n","If I confess it,--this is such a pleasure to-night: but he will not come and sinkens:\n","I dreaming on your deeds to my note, that would twickary wash: and ill, I had not.\n","\n","MISTRESS QUICKLY:\n","And with this tail'd bid thy life way to die howest that he indeed, the carry is obedient.\n","\n","KING LEAR:\n","You are as first and consoligence, and take them not, there's a thoughts: the carriest?\n","\n","PROSPERO:\n","O sons are now hasten the silence,\n","And his course and confusing soldiers.\n","\n","MISTRESS PAGE:\n","We shall prote those that wife is craves to cry. Young like anong, and to despise his counsel and breast,\n","That I'll speed and my slave as sleep his hand; that's all our hands to too lined things is.\n","\n","PISTOL:\n","When my friend! indeed a sworn cheer to thy place with wine!\n","\n","POINS:\n","I have now bold in purpose: and have now as is my husband's chamberley-fiench is adventure\n","And that double scape thus condition. You have patient sir, we have dreadfully:\n","You all they say altistains with the partious contempt of this lord begins,\n","Thou shalt not blob not when I come bether a dreaden was and concluded.\n","\n","ACHILLES:\n","And would not think thou behold our worshion since, stand.\n","Birn beeping me, worthy that.\n","\n","RICHMOND SALE:\n","Boy will long, so it come to me.\n","\n","AARON:\n","Mine eyes shall summer bids you for your spleess scright fair life in thee, the heast direction: but.\n","\n","AUTOLYCUS:\n","O, my lord and purpose insminent their forbeard,\n","That you shall beat his heaven servant took me who lies me, have no more\n","The scandoms boy, or thy father's dangerous worship she still cannot hard my ground.\n","Thy lords, sir, I'll promise our pleasure, and then music fell the peeped fair, my man, and send.\n","\n","ALCIBIADES:\n","What should he but a sword, if this? shall trust, shall belief me for and bid this truly to-morrow.\n","\n","Servaint:\n","Not desire you, I am not as I heard,\n","Their song the earth theil. But you both, for the best of wents, and had: if he think against thy forty-streed slaughter doth serves it.\n","\n","MICHBION:\n","Sir, the changer'd?\n","\n","SICINIUS:\n","Why, my madam and mine?\n","\n","LAFEU:\n","I made my master to coming your full for him to my lord,\n","I must that he say shall rascan. But at the world shines and cracks doubt you towest, for thanks and digning but thee within,\n","And defend her tender and bower.\n","\n","LADY MACDUFF:\n","Near your birthin look of wrong is instant of your life,\n","Wit in sensible words, and bet of heaven way the masker of an indeed of this proud wit.\n","The comfort; for would dark other wish'd conceal birth of my cross with me with it\n","And had the purchinable after we made me.\n","\n","MISTRESS FORD:\n","Sir Johmustrard!\n","You'll murrebody.\n","\n","HIOLLIUS:\n","Now, like attired, any face\n","To cithee, like the churchiment did.\n","\n","ARVIRAGUS:\n","My lord, my form's by nore, and beseech you that is:\n","Out it, I told me thy face of this son, we list with thee forgot of a good mattor show'd wherewithed,\n","By spurst.\n","\n","Messenger:\n","What, if thou saw with my father of you, it is\n","Descreft and call the maddless of me; and thou please in a poxcy badgise his sairs,\n","That hath this matched, with the smile is allieus those than with old malile; the first which me before my life h\n"]}]},{"cell_type":"markdown","source":["Finally, implement temperature-based decoding. How do the generations change when you change the temperature parameter? How does it compare to the previous sampling methods?"],"metadata":{"id":"y_gfej2ngGgh"}},{"cell_type":"code","source":["# TODO: implement temperature-based decoding here\n","# Temperature sampling:\n","# 1 Divide the logit (the input to the softmax) by a temperature parameter\n","# y =softmax(u/Ï„)\n","\n","# The context that we prompt the model with:\n","prompt = \"JULIET:\"\n","\n","# translate the prompt into indices\n","# iterate over our training data using the char2index dictionary\n","# identify the character that matches our prompt\n","# add this to a list of output_indices\n","output_indices = [train_data.char2index[c] for c in prompt]\n","\n","# turn the list into a PyTorch tensor and move it to the GPU, if available\n","# Multi-dimensional matrix containing elements of a single data type, specifically 64-bit signed integers.\n","# This tensor type is often used for indexing and other operations that require integer values.\n","prev_chars = torch.LongTensor([output_indices]).to(device)\n","\n","# Initialise the hidden state\n","hidden = None\n","\n","# how many characters to generate\n","# capping at specific value because we didn't implement a stop token\n","output_len = 5000\n","\n","# set the model to evaluation mode\n","model.eval()\n","# iterate over values in output_len\n","for i in range(output_len):\n","\n","  # compute the logits for the next character\n","  logits, hidden = model(prev_chars, hidden)\n","\n","  logits = top_k_filtering(logits, top_k= 12)\n","\n","  # turn them into probabilities\n","  probs = F.softmax(logits, dim=-1/)\n","\n","  # torch.multinomial = Returns a tensor where each row contains num_samples indices sampled from the multinomial\n","  prev_chars = torch.multinomial(probs, 1).view(1,1)\n","\n","  # add the character to the list\n","  # we'll have to move it back to the CPU, if we are using a GPU and convert it from\n","  # the pytorch format to a regular integer\n","  output_indices.append(prev_chars.cpu().view(1).tolist()[0])\n","\n","# Let's print the generation:\n","print_output_indices(output_indices)"],"metadata":{"id":"dO8QC6S1gTBU"},"execution_count":null,"outputs":[]}]}