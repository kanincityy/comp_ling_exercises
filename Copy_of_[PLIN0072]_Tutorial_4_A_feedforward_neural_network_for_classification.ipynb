{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanincityy/comp_ling_exercises/blob/main/Copy_of_%5BPLIN0072%5D_Tutorial_4_A_feedforward_neural_network_for_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "BzcXwmAImT3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e54f4c2d-409f-4615-e8cf-5e0fc527bd88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting otter-grader\n",
            "  Downloading otter_grader-6.1.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (8.1.8)\n",
            "Collecting dill>=0.3.0 (from otter-grader)\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fica>=0.4.1 (from otter-grader)\n",
            "  Downloading fica-0.4.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting ipylab<2.0.0,>=1.0.0 (from otter-grader)\n",
            "  Downloading ipylab-1.0.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from otter-grader) (7.34.0)\n",
            "Collecting ipywidgets<9.0.0,>=8.1.5 (from otter-grader)\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: jinja2<4.0,>=3.1 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (3.1.5)\n",
            "Collecting jupytext<2.0.0,>=1.16.4 (from otter-grader)\n",
            "  Downloading jupytext-1.16.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: nbconvert>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (7.16.6)\n",
            "Requirement already satisfied: nbformat>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (5.10.4)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (2.2.2)\n",
            "Collecting python-on-whales<1.0.0,>=0.72.0 (from otter-grader)\n",
            "  Downloading python_on_whales-0.75.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pyyaml<7,>=6 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (6.0.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (2.32.3)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from otter-grader) (1.17.2)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.11/dist-packages (from fica>=0.4.1->otter-grader) (0.21.2)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.11/dist-packages (from fica>=0.4.1->otter-grader) (8.1.3)\n",
            "Collecting comm>=0.1.3 (from ipywidgets<9.0.0,>=8.1.5->otter-grader)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9.0.0,>=8.1.5->otter-grader) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets<9.0.0,>=8.1.5->otter-grader)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets<9.0.0,>=8.1.5->otter-grader) (3.0.13)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython->otter-grader)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->otter-grader) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0,>=3.1->otter-grader) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=1.0 in /usr/local/lib/python3.11/dist-packages (from jupytext<2.0.0,>=1.16.4->otter-grader) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from jupytext<2.0.0,>=1.16.4->otter-grader) (0.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from jupytext<2.0.0,>=1.16.4->otter-grader) (24.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (3.1.1)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (1.5.1)\n",
            "Collecting playwright (from nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader)\n",
            "  Downloading playwright-1.50.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.0.0->otter-grader) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.0.0->otter-grader) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->otter-grader) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->otter-grader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->otter-grader) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->otter-grader) (2025.1)\n",
            "Requirement already satisfied: pydantic!=2.0.*,<3,>=2 in /usr/local/lib/python3.11/dist-packages (from python-on-whales<1.0.0,>=0.72.0->otter-grader) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from python-on-whales<1.0.0,>=0.72.0->otter-grader) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->otter-grader) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->otter-grader) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->otter-grader) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->otter-grader) (2025.1.31)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (1.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->otter-grader) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.0.0->otter-grader) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (4.3.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=1.0->jupytext<2.0.0,>=1.16.4->otter-grader) (0.1.2)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from nbclient>=0.5.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (6.1.12)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->otter-grader) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,<3,>=2->python-on-whales<1.0.0,>=0.72.0->otter-grader) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,<3,>=2->python-on-whales<1.0.0,>=0.72.0->otter-grader) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->otter-grader) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (2.6)\n",
            "Collecting pyee<13,>=12 (from playwright->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader)\n",
            "  Downloading pyee-12.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (3.1.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.0.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx->fica>=0.4.1->otter-grader) (1.4.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (24.0.1)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert>=6.0.0->nbconvert[webpdf]>=6.0.0; sys_platform != \"emscripten\" and sys_platform != \"wasi\"->otter-grader) (6.4.2)\n",
            "Downloading otter_grader-6.1.0-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.3/142.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fica-0.4.1-py3-none-any.whl (13 kB)\n",
            "Downloading ipylab-1.0.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.2/100.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupytext-1.16.6-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.2/154.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_on_whales-0.75.1-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.0/115.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading playwright-1.50.0-py3-none-manylinux1_x86_64.whl (45.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-12.1.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: widgetsnbextension, pyee, jedi, dill, comm, playwright, python-on-whales, ipywidgets, fica, ipylab, jupytext, otter-grader\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed comm-0.2.2 dill-0.3.9 fica-0.4.1 ipylab-1.0.0 ipywidgets-8.1.5 jedi-0.19.2 jupytext-1.16.6 otter-grader-6.1.0 playwright-1.50.0 pyee-12.1.1 python-on-whales-0.75.1 widgetsnbextension-4.0.13\n",
            "--2025-02-08 16:02:00--  https://sebschu.com/files/plin0072/tutorials/tutorial4/tests.zip\n",
            "Resolving sebschu.com (sebschu.com)... 185.199.109.153, 185.199.110.153, 185.199.111.153, ...\n",
            "Connecting to sebschu.com (sebschu.com)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3395 (3.3K) [application/zip]\n",
            "Saving to: ‘tests.zip’\n",
            "\n",
            "tests.zip           100%[===================>]   3.32K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-08 16:02:00 (36.1 MB/s) - ‘tests.zip’ saved [3395/3395]\n",
            "\n",
            "Archive:  tests.zip\n",
            "   creating: tests/\n",
            "  inflating: tests/q1.py             \n",
            "  inflating: tests/q4.py             \n",
            "  inflating: tests/q3.py             \n",
            "  inflating: tests/q2.py             \n"
          ]
        }
      ],
      "source": [
        "!pip install otter-grader\n",
        "!wget \"https://sebschu.com/files/plin0072/tutorials/tutorial4/tests.zip\" && unzip -o tests.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "UoW6xybemT3a"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMJK-YJ0RGvK"
      },
      "source": [
        "# Tutorial 4: A feedforward neural network for classification\n",
        "\n",
        "In this tutorial, you will implement your first neural network. We'll once again do this in the context of a part-of-speech tagger.\n",
        "\n",
        "### Setup\n",
        "\n",
        "Please first make a copy of this notebook into your personal Google Drive. You can do so by clicking on _File_ (top left corner) and choosing \"Save a copy in Drive\". Then, make sure that you run the two cells on top of the notebook before running any of the other cells. Otherwise you will not be able to run the automatic tests.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUY7Rqz0hJvb"
      },
      "source": [
        "## A feedforward neural network for part-of-speech taggging\n",
        "\n",
        "As we've discussed in Lectures 3 and 4, we can use feedforward neural networks for building multilabel classifiers (instead of the multinomial logistic regression classifiers that we've been using so far).\n",
        "\n",
        "To do this, we'll need two major things (along with lots of little bits, many of which are already provided in this notebook).\n",
        "\n",
        "1. We need to define a dataset class that builds a datastructure for the examples for us.\n",
        "\n",
        "2. We need to define the structure of the neural network model.\n",
        "\n",
        "This notebook will walk you through on how to do these things. In order to keep this manageable, I've already provided quite a bit of skeleton code to get you started. I'd recommend you still try to understand what I've done in the code that I already provided and I'd even suggest you try to implement your own network for a different task after you've completed this tutorial to get a sense of which moving parts are required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "AmP-xqaov--m"
      },
      "source": [
        "### Your task: Loading word vectors\n",
        "\n",
        "Run the following cell to download the GloVe embeddings to the folder `glove`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wx4HHt5SaqEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdc955e7-0644-4856-8049-af8153d8049b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-08 16:14:08--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-02-08 16:14:08--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.05MB/s    in 2m 40s  \n",
            "\n",
            "2025-02-08 16:16:49 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "# Download the embeddings trained on 6B tokens from Wikipedia and news texts.\n",
        "! mkdir -p glove && cd glove && wget https://nlp.stanford.edu/data/glove.6B.zip && unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "T0FGg_cYwUm5"
      },
      "source": [
        "We'll once again start by loding the word embeddings as we've done in previous weeks.  We'll use the 300-dimensional embeddings stored in `glove/glove.6B.300d.txt`.\n",
        "\n",
        "However, this week, I'd like you to add 2 vectors to the embedding matrix after you've loaded it:\n",
        "\n",
        "1. An unknown vector. This vector should be in the second-to-last row of the embedding matrix and its value should be the mean of all vectors in the embedding matrix. Add an entry with the key `\"-UNK-\"` to `word2index` with the correct row in the embedding matrix.\n",
        "\n",
        "2. A zero vector for missing words. This vector should be in the last row of the embedding matrix and its value should be a vector with all zeros. Add an entry with the key `\"-NONE-\"` to `word2index` with the correct row in the embedding matrix.\n",
        "\n",
        "The final embedding matrix should then be of shape `(400002, 300)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fJv7qU8a7PX",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3daf3b-59b5-4b7a-a245-0f0e651611b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final embedding matrix shape: (400002, 300)\n",
            "word2index has 400002 entries.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# TODO: load the embeddings\n",
        "NO_OF_DIMENSIONS = 300\n",
        "import numpy as np\n",
        "\n",
        "# TODO load the embeddings\n",
        "NO_OF_DIMENSIONS = 300\n",
        "cols_to_use = list(range(1, NO_OF_DIMENSIONS + 1))\n",
        "embeddings = np.loadtxt(\"glove/glove.6B.300d.txt\", encoding=\"UTF-8\", usecols=cols_to_use, comments=None)\n",
        "\n",
        "word2index = {}\n",
        "\n",
        "# TODO: populate the word2index dictionary\n",
        "with open(\"glove/glove.6B.100d.txt\", \"r\", encoding=\"UTF-8\") as embedding_f:\n",
        "  # this loops through the file and returns the line number in the file and the\n",
        "  # contents of the line on each iteration\n",
        "  for i, line in enumerate(embedding_f):\n",
        "    cols = line.split(\" \") # let's split the line into columns\n",
        "    word = cols[0] # the first column contains the word\n",
        "    word2index[word] = i # add the word to the mapping\n",
        "\n",
        "# Convert embeddings list to a numpy array\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "# Compute the -UNK- vector (mean of all vectors)\n",
        "UNK_VECTOR = np.mean(embeddings, axis=0)\n",
        "\n",
        "# Create the -NONE- vector (a zero vector)\n",
        "NONE_VECTOR = np.zeros(NO_OF_DIMENSIONS, dtype=np.float32)\n",
        "\n",
        "# Add -UNK- and -NONE- to word2index\n",
        "word2index[\"-UNK-\"] = len(embeddings)\n",
        "word2index[\"-NONE-\"] = len(embeddings) + 1\n",
        "\n",
        "# Add -UNK- and -NONE- vectors to the embedding matrix\n",
        "embeddings = np.vstack([embeddings, UNK_VECTOR, NONE_VECTOR])\n",
        "\n",
        "# Verify the final embedding matrix shape\n",
        "print(f\"Final embedding matrix shape: {embeddings.shape}\")\n",
        "print(f\"word2index has {len(word2index)} entries.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "urkBfGJJmT3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "ca961acf-11b6-47eb-e0d1-10785f72d571"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "q1 results: All test cases passed!"
            ],
            "text/html": [
              "<p><strong><pre style='display: inline;'>q1</pre></strong> passed! 🙌</p>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "grader.check(\"q1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zj4SxVtW1cQf"
      },
      "source": [
        "### Implementing the Dataset\n",
        "\n",
        "The first thing we'll have to do is implement a dataset class. This is a class that will massage the data we have into the right format so that we can input it to the neural network.\n",
        "\n",
        "A dataset class is somewhat similar to a feature extractor, but we rarely do anything more sophisticated than figuring out what the indices of the words in an embedding matrix should be.\n",
        "\n",
        "As usual, we'll first have to read in the data from the CoNLL-U files. Run the following two cells to define the `read_conllu_file` function and download the ConLL-U files, as we've been doing before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7dYEfgvydH19"
      },
      "outputs": [],
      "source": [
        "# A function to read in CoNLL-U files, which contain sentences annotated with\n",
        "# their part-of-speech tags.\n",
        "# See https://universaldependencies.org/format.html for more info on this file\n",
        "# format.\n",
        "\n",
        "def read_conllu_file(file_path):\n",
        "  sentences = []\n",
        "  unique_labels = set()\n",
        "\n",
        "  with open(file_path, \"r\", encoding=\"UTF-8\") as in_f:\n",
        "    current_sentence = []\n",
        "    for line in in_f:\n",
        "      line = line.strip()\n",
        "      # ignore lines starting with # (comments)\n",
        "      if line.startswith(\"#\"):\n",
        "        continue\n",
        "\n",
        "      # an empty line indicates the end of the sentence\n",
        "      if line == \"\":\n",
        "        sentences.append(current_sentence)\n",
        "        current_sentence = []\n",
        "        continue\n",
        "\n",
        "      # split the line into its parts\n",
        "      parts = line.split(\"\\t\")\n",
        "\n",
        "      # extract the index (the first column)\n",
        "      idx = parts[0]\n",
        "\n",
        "      # check if this is a multi-word token or an empty node\n",
        "      if \".\" in idx or \"-\" in idx:\n",
        "        continue\n",
        "\n",
        "      if len(parts) < 4:\n",
        "        print(parts)\n",
        "      # extract the word and the tag, i.e., the second and fourth column\n",
        "      word, tag = parts[1], parts[3]\n",
        "\n",
        "      unique_labels.add(tag)\n",
        "\n",
        "      # append the word, tag pair to the current sentence\n",
        "      current_sentence.append((word, tag))\n",
        "\n",
        "  return sentences, unique_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2CIQI3_Mcytl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3538c7-3d73-4c57-b943-6e25638c9325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-08 16:19:06--  https://github.com/UniversalDependencies/UD_English-EWT/raw/refs/heads/master/en_ewt-ud-train.conllu\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/refs/heads/master/en_ewt-ud-train.conllu [following]\n",
            "--2025-02-08 16:19:06--  https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/refs/heads/master/en_ewt-ud-train.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14190227 (14M) [text/plain]\n",
            "Saving to: ‘en_ewt-ud-train.conllu’\n",
            "\n",
            "en_ewt-ud-train.con 100%[===================>]  13.53M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-02-08 16:19:07 (266 MB/s) - ‘en_ewt-ud-train.conllu’ saved [14190227/14190227]\n",
            "\n",
            "--2025-02-08 16:19:07--  https://github.com/UniversalDependencies/UD_English-EWT/raw/refs/heads/master/en_ewt-ud-dev.conllu\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/refs/heads/master/en_ewt-ud-dev.conllu [following]\n",
            "--2025-02-08 16:19:08--  https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/refs/heads/master/en_ewt-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1804172 (1.7M) [text/plain]\n",
            "Saving to: ‘en_ewt-ud-dev.conllu’\n",
            "\n",
            "en_ewt-ud-dev.conll 100%[===================>]   1.72M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-08 16:19:09 (178 MB/s) - ‘en_ewt-ud-dev.conllu’ saved [1804172/1804172]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download annotated files. This downloads files for English but you can also change the paths to download files for other languages.\n",
        "# See the repositories in https://github.com/UniversalDependencies for other languages.\n",
        "! mkdir -p data && cd data && wget https://github.com/UniversalDependencies/UD_English-EWT/raw/refs/heads/master/en_ewt-ud-train.conllu\n",
        "! cd data && wget https://github.com/UniversalDependencies/UD_English-EWT/raw/refs/heads/master/en_ewt-ud-dev.conllu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "hT2vMtNSIjMN"
      },
      "source": [
        "### Your task: building a label2index dictionary\n",
        "\n",
        "As a step towards building the Dataset class, we'll need a `label2index` dictionary which maps the part-of-speech tag to an index. The exact index does not matter, but assuming that there are $k$ unique part of speech tags in our training data, each part of speech tag should be assigned a unique number between 0 and $k-1$.\n",
        "\n",
        "For example, if the only part-of-speech tags are \"NOUN\", \"VERB\", and \"ADJ\", then one possible `label2index` would be:\n",
        "\n",
        "```python\n",
        "  label2index = {\n",
        "    \"NOUN\": 0,\n",
        "    \"VERB\": 1,\n",
        "    \"ADJ\": 2\n",
        "  }\n",
        "```\n",
        "\n",
        "You can use th set `unique_labels` that contains all the unique POS tags in the training data to construct this index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2_gqj7Z9X6b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_data_sentences, unique_labels = read_conllu_file(\"data/en_ewt-ud-train.conllu\")\n",
        "label2index = {}\n",
        "\n",
        "# TODO: Populate `label2index`\n",
        "label2index = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
        "\n",
        "# print(label2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ohjYuDtQmT3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "e09aa211-07fb-4664-b38d-423999b7d2ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "q2 results: All test cases passed!"
            ],
            "text/html": [
              "<p><strong><pre style='display: inline;'>q2</pre></strong> passed! 💯</p>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "grader.check(\"q2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "hvYb_OtLDdMu"
      },
      "source": [
        "### Your task: Complete the Dataset Class\n",
        "\n",
        "The following cell already contains the skeleton of a class `POSDataset` that inherits from the `torch.utils.data.Dataset` class.\n",
        "\n",
        "A dataset class needs to implement at least two methods:\n",
        "* the `__len__(self)` method returns the number of examples in the dataset\n",
        "* the `__getitem__(self, index)` method that returns the `index`-th example\n",
        "\n",
        "In this skeleton, I've already implemented the constructor (the `__init__` method), a method `get_word_idx` that returns the index of a word in the embedding matrix, and the `__len__(self)` and the `__getitem__(self, index)` methods.\n",
        "\n",
        "Your job is to implement the `extract_examples` method. This method should build two lists:\n",
        "\n",
        "* `self.labels` which contains a list of the indices of the POS tag for each example (as stored in `label2index`)\n",
        "* `self.examples` which contains a list of lists of the word indices within the context window. We'll consider a window size of 5 here, that is we include the two words right before the word that we want to tag, the word that we want to tag, and the two word after the one we want to tag.\n",
        "\n",
        "For any word that does not exist (e.g., because the target word is the first word of the sentence and there are no previous words, or the target word is the second-to-last word of the sentence and there is no word 2 positions after the target word), use the index of the special `-NONE-` word that you've created above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4zNcYr8bdbE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "    \"\"\"Loads POS tagging dataset to be used in a feed-forward neural network.\"\"\"\n",
        "\n",
        "    # The constructor which takes in the list of sentences\n",
        "    # returned by read_connlu_file and the word2index and label2index\n",
        "    # dictionaries.\n",
        "    def __init__(self, sentences, word2index, label2index):\n",
        "      # Initialise the class instances with dataset\n",
        "        self.sentences = sentences\n",
        "        self.word2index = word2index\n",
        "        self.label2index = label2index\n",
        "        self.extract_examples()\n",
        "\n",
        "    # this is a method that returns the index of a word\n",
        "    # using the word2index dictionary.\n",
        "    # It returns the index of the \"-UNK-\" token\n",
        "    # if there is no embedding for the word.\n",
        "    #\n",
        "    # You can call this method from other methods of this class\n",
        "    # using self.get_word_idx(word)\n",
        "    def get_word_idx(self, word):\n",
        "      word = word.lower()\n",
        "      if word in self.word2index:\n",
        "        return self.word2index[word]\n",
        "      elif word == \"-NONE-\":\n",
        "        # Explicitly handle the \"-NONE-\" token\n",
        "        return self.word2index[\"-NONE-\"]\n",
        "      else:\n",
        "        # Default to \"-UNK-\"\n",
        "        return self.word2index[\"-UNK-\"]\n",
        "\n",
        "    # this method should populate self.examples\n",
        "    # and self.labels\n",
        "    def extract_examples(self):\n",
        "      window_size = 2\n",
        "      self.examples = [] # list of lists each entry corresponds to 1 example\n",
        "      self.labels = [] # index of label corresponding to example\n",
        "\n",
        "      # Iterate through sentence in dataset\n",
        "      for sentence in self.sentences:\n",
        "        # Define n_words for each sentence length\n",
        "        n_words = len(sentence)\n",
        "        # Unpack words and labels from sentence\n",
        "        words, labels = zip(*sentence)\n",
        "        # Iterate through index of words\n",
        "        for i in range(n_words):\n",
        "          # Build context window, creates a cohesive example for each target word!\n",
        "          context_window = []\n",
        "          # TODO: populate self.examples and self labels\n",
        "          # as described above\n",
        "          # Iterate words within context window range\n",
        "          for j in range(i-window_size, i+window_size+1):\n",
        "            if j < 0 or j >= n_words:\n",
        "              # If word out of bounds, use '-NONE-' token\n",
        "              context_window.append(self.get_word_idx(\"-NONE-\"))\n",
        "            else:\n",
        "              # If valid get index with get_word_idx function\n",
        "              context_window.append(self.get_word_idx(words[j]))\n",
        "\n",
        "          # Add context window to examples list\n",
        "          self.examples.append(context_window)\n",
        "          # Add label index = Look into how we access this!!!\n",
        "          self.labels.append(self.label2index[labels[i]])\n",
        "\n",
        "    # this method returns the number of examples\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    # this method returns one training example\n",
        "    # Note that it converts everything into torch.Tensors.\n",
        "    # These objects are ways to represent matrices in the pytorch internal format.\n",
        "    def __getitem__(self, index):\n",
        "        input_ids = torch.LongTensor([self.examples[index]])\n",
        "        label_id = torch.LongTensor([self.labels[index]])\n",
        "        return input_ids, label_id\n",
        "\n",
        "train_dataset = POSDataset(train_data_sentences, word2index, label2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "SnR33A0UmT3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "9c50c94b-37c5-42bd-c404-796bf9b97b1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "q3 results:\n",
              "    q3 - 1 result:\n",
              "        ❌ Test case failed\n",
              "        Error at line 48 in test q3:\n",
              "              assert x1.tolist() == [[400001, 400001, 318, 11, 34408]], test_dataclass_desc(4, x1.tolist(), \"x1.tolist()\")\n",
              "        AssertionError: \n",
              "\n",
              "        The last line of the following test program failed.\n",
              "        Make sure that your function returns exactly the same value\n",
              "        as specified in the <b>assert</b> statement.\n",
              "\n",
              "        assert len(train_dataset) == 204579\n",
              "        x1, y1 = train_dataset[0]\n",
              "        x2, y2 = train_dataset[1]\n",
              "        <b>assert x1.tolist() == [[400001, 400001, 318, 11, 34408]]</b>\n",
              "\n",
              "        x1.tolist() returned:\n",
              "        [[400000, 400000, 318, 11, 34408]]"
            ],
            "text/html": [
              "<p><strong style='color: red;'><pre style='display: inline;'>q3</pre> results:</strong></p><p><strong><pre style='display: inline;'>q3 - 1</pre> result:</strong></p><pre>    ❌ Test case failed\n",
              "    Error at line 48 in test q3:\n",
              "          assert x1.tolist() == [[400001, 400001, 318, 11, 34408]], test_dataclass_desc(4, x1.tolist(), \"x1.tolist()\")\n",
              "    AssertionError: \n",
              "\n",
              "    The last line of the following test program failed.\n",
              "    Make sure that your function returns exactly the same value\n",
              "    as specified in the <b>assert</b> statement.\n",
              "\n",
              "    assert len(train_dataset) == 204579\n",
              "    x1, y1 = train_dataset[0]\n",
              "    x2, y2 = train_dataset[1]\n",
              "    <b>assert x1.tolist() == [[400001, 400001, 318, 11, 34408]]</b>\n",
              "\n",
              "    x1.tolist() returned:\n",
              "    [[400000, 400000, 318, 11, 34408]]\n",
              "\n",
              "</pre>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "grader.check(\"q3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "yk5Hjoo16x21"
      },
      "source": [
        "### Your task: Defining the model\n",
        "\n",
        "Now finally, you'll have to define the model. You are expected to implement a neural network with one hidden layer here.\n",
        "\n",
        "This involves two parts:\n",
        "\n",
        "1. In the constructor (the `__init__` method) you have to define everything that has parameters, i.e.,  that has weight matrices.\n",
        "\n",
        "2. In the forward function, you have to define how the final output value is computed.\n",
        "\n",
        "I'd recommend looking at the code from [last week's lecture colab](https://colab.research.google.com/drive/1F95UO_3Vry8LExPce-wfltIeIYEWBINP#scrollTo=FbzW2JeADxXN) for how to do this.\n",
        "\n",
        "In both functions, I've already defined the embedding layer. So you only have to process the data starting with the vector of 5 concanated word vectors.\n",
        "\n",
        "Make sure to define the size of the layers based on the constructor variables `input_dimension`, `hidden_dimension`,  and `output_dimension`.\n",
        "\n",
        "Also note that the loss function, the cross entropy loss, does not require the final step to use the softmax function. The `forward` function can therefore return the value of what you would pass through the `softmax` function instead of a list of probabilties.\n",
        "\n",
        "Finally, we typically process a small number of examples at the same time, in so-called batches. Pytorch is quite good at automatically handling this but, for example, when we compute the input using the embedding matrix, we have to consider this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWDwu9_bajuv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Look at lecture colab from Week 3\n",
        "class FeedForwardTagger(nn.Module):\n",
        "  # init defines everything that has a weight\n",
        "  def __init__(self, input_dimension, hidden_dimension, output_dimension, embeddings):\n",
        "    super().__init__()\n",
        "\n",
        "    # This defines the embedding layer and intialises it with an existing embedding matrix\n",
        "    self.embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embeddings).to(dtype=torch.float32))\n",
        "\n",
        "    # TODO: Define the remaining layers here\n",
        "    self.hidden_layer = nn.Linear(input_dimension, hidden_dimension)\n",
        "    self.output_layer = nn.Linear(hidden_dimension, output_dimension)\n",
        "\n",
        "    # Dropout layers\n",
        "    self.dropout1 = nn.Dropout(0.5)\n",
        "\n",
        "  # the input x is a matrix of batch_size rows with 5 word indices in each row\n",
        "  # defines computations with components from init function\n",
        "  # Forward method defines the data flow through the network\n",
        "  def forward(self, x):\n",
        "    # The following two lines embed the tokens\n",
        "    input = self.embedding_layer(x) # returns a tensor with shape batch_size x embedding_dimension\n",
        "    # This turns embedding_dimension matrix into a single vector of embeddings\n",
        "    input = input.flatten(start_dim=1) # shape: batch_size x embedding_dimension\n",
        "\n",
        "    # TODO: compute h and the output here\n",
        "    h = F.relu(self.hidden_layer(input)) # shape: batch_size x hidden_dimension\n",
        "    output = self.output_layer(h) # shape: batch_size x output_dimension\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "QUoXpK4wmT3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "a897ed9a-1523-433c-dd65-c968e1059ba2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "q4 results: All test cases passed!"
            ],
            "text/html": [
              "<p><strong><pre style='display: inline;'>q4</pre></strong> passed! ✨</p>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "grader.check(\"q4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euztws_GhN53"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "Now that we've defined everything, we are ready to train the model.\n",
        "\n",
        "The code below defines the model, the loss function, the optimizer and then a `train` function that runs the training process for a set number of epochs.\n",
        "\n",
        "Run the following two blocks to train your neural network. If you implemented everything correctly, you should see your loss go down with (almost) every iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rJwg8BZ_xK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77945840-21e5-440a-cfae-b305f247719e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# The following few lines check whether a GPU is available, and if so,\n",
        "# they run everything on a GPU which will be much faster.\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Let's define the batch_size, the window size, the output size, and the size of the hidden layer\n",
        "# Experiment with different sizes for hidden layer, etc. to optimise model\n",
        "# Consider adding second layer?\n",
        "batch_size = 64\n",
        "window_size = 5\n",
        "output_size = len(label2index)\n",
        "hidden_size = 200\n",
        "\n",
        "# Let's define the model and move it to the GPU, if available.\n",
        "model = FeedForwardTagger(window_size * NO_OF_DIMENSIONS, hidden_size, output_size, embeddings).to(device)\n",
        "# We use a cross-entropy loss here\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "# Finally, let's define the optimiser\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "\n",
        "# This loads the data for training\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# The function for running model training\n",
        "def train(model, loss_fn, optimizer, dataloader, epochs=150):\n",
        "    # number of times we go through the training data\n",
        "    # set the mode of the model to training so that parameters are being updated\n",
        "    model.train()\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        # intialise the loss at the beginning of each epoch\n",
        "        epoch_loss = 0\n",
        "\n",
        "        # iterate through all training batches\n",
        "        for X, y in dataloader:\n",
        "\n",
        "            # move the input and the labels to the GPU, if we are using a GPU\n",
        "            X = X.to(device)\n",
        "            y = y.to(device).flatten()\n",
        "\n",
        "            # reset the optimiser\n",
        "            optimizer.zero_grad()\n",
        "            # compute the current model predictions for the input X\n",
        "            # this calls the forward function of the model\n",
        "            y_hat = model(X)\n",
        "            # compute the loss for the current predictions\n",
        "            loss = loss_fn(y_hat, y)\n",
        "            # perform backpropagation\n",
        "            loss.backward()\n",
        "            # and update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # add the loss of the current batch to the loss of the epoch\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # print the loss at the end of every epoch\n",
        "        print(\"Epoch: {0}, Loss: {1}, \".format(i, epoch_loss))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez9GhgzJif_V"
      },
      "source": [
        "Run the following cell to start the training process. You can also increase the number of epochs, which should improve your model but it will also take longer to run the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7K4OS-1EQBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbf4ec1-e2b4-4212-c82f-58169d17b520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1646.0208383053541, \n",
            "Epoch: 1, Loss: 866.9893937073648, \n",
            "Epoch: 2, Loss: 598.9228148944676, \n",
            "Epoch: 3, Loss: 440.39656713046134, \n",
            "Epoch: 4, Loss: 332.0264025107026, \n",
            "Epoch: 5, Loss: 249.4213908817619, \n",
            "Epoch: 6, Loss: 191.3998173205182, \n",
            "Epoch: 7, Loss: 146.97759441216476, \n",
            "Epoch: 8, Loss: 121.01833791239187, \n",
            "Epoch: 9, Loss: 98.57993226801045, \n",
            "Epoch: 10, Loss: 83.37917726230808, \n",
            "Epoch: 11, Loss: 73.1302848883206, \n",
            "Epoch: 12, Loss: 67.14821660402231, \n",
            "Epoch: 13, Loss: 60.23000479664188, \n",
            "Epoch: 14, Loss: 58.024878251279006, \n"
          ]
        }
      ],
      "source": [
        "train(model, loss_fn, optimizer, train_dataloader, epochs=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc9cV3Ah65HY"
      },
      "source": [
        "### Evaluating the model\n",
        "\n",
        "Finally, let's evaluate how well the model does on the test data.\n",
        "\n",
        "The following three cells construct the test dataset, then use the model go run predictions, and finally  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiAHfNecAtzh"
      },
      "outputs": [],
      "source": [
        "# load the test sentences from the conllu file\n",
        "test_data_sentences, _ = read_conllu_file(\"data/en_ewt-ud-dev.conllu\")\n",
        "\n",
        "# construct the test dataset from the test sentences\n",
        "test_dataset = POSDataset(test_data_sentences, word2index, label2index)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc5DMuCMdvbm"
      },
      "outputs": [],
      "source": [
        "# this tells the model that it should only perform predictions and not update any weights\n",
        "model.eval()\n",
        "\n",
        "preds = []\n",
        "true_labels = []\n",
        "# loop through all batches in the test datseet\n",
        "for X, y in test_dataloader:\n",
        "\n",
        "  # move X to the correct device (the GPU if it exists)\n",
        "  X = X.to(device)\n",
        "\n",
        "  # make predictions by calling model(X) and move them back to the cpu, if we are using a GPU\n",
        "  model_prediction = model(X).cpu()\n",
        "\n",
        "  # this gets the index of the top 1 prediction, i.e., the class index of the most probable class\n",
        "  y_hat = model_prediction.topk(1)[1]\n",
        "\n",
        "  # add the true label and the predictions to the respective lists\n",
        "  true_labels.extend(y.tolist())\n",
        "  preds.extend(y_hat.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4G-vZ04eTay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e16365a-f3c0-4684-b17a-fee130d399a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 91.77%\n"
          ]
        }
      ],
      "source": [
        "# compute the accuracy\n",
        "\n",
        "correct = 0\n",
        "n = len(true_labels)\n",
        "for i in range(n):\n",
        "  if true_labels[i] == preds[i]:\n",
        "    correct +=1\n",
        "\n",
        "print(f\"Test accuracy: {correct / n * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HiDF19xj9j8"
      },
      "source": [
        "### Additional explorations\n",
        "\n",
        "Once you've completed these steps, you can also experiment with optimising the model. For example, you can change the hyperparameters, these are parameters such as the dimension of the hidden layer or the window size and see how it affects model performance. You can also try training the model for more or fewer epochs and see how that affects performance. Or you can experiment with the model architecture, and for example, add additional hidden layers to the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "otter": {
      "assignment_name": "tutorial4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}